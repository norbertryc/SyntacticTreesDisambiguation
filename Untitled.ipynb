{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vecs_full = pickle.load(open(\"embeddings/w2v_allwiki_nkjpfull_300.pkl\",\"rb\"),encoding=\"latin1\")\n",
    "\n",
    "datasets = [\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/tokens.txt\"]\n",
    "\n",
    "words = set()\n",
    "for dataset in datasets:\n",
    "    with open(dataset) as f:\n",
    "        for x in f.read().split():\n",
    "            words.add(x)\n",
    "            \n",
    "words2ids_filtered = {}\n",
    "vectors_filtered = []\n",
    "\n",
    "i = 0\n",
    "for word in w2vecs_full.index2word:\n",
    "    if word in words:\n",
    "        words2ids_filtered[word] = i\n",
    "        vectors_filtered.append(w2vecs_full[word])\n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "t = open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/tokens.txt\",\"r\")\n",
    "tokens = [x.split() for x in t.readlines()]\n",
    "t.close()\n",
    "r = open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/rules.txt\",\"r\")\n",
    "rules = [x.split() for x in r.readlines()]\n",
    "r.close()\n",
    "\n",
    "\n",
    "tokens2 = [[]]*len(tokens)\n",
    "unique_rules = set()\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        if tokens[i][j][:2] == \"__\":\n",
    "            tokens2[i].append(tokens[i][j]+rules[i][j])\n",
    "            unique_rules.add(tokens[i][j]+rules[i][j])\n",
    "        else:\n",
    "            tokens2[i].append(tokens[i][j])\n",
    "            \n",
    "            \n",
    "for i, x in zip(range(len(vectors_filtered), len(vectors_filtered) + len(unique_rules)), unique_rules):\n",
    "    \n",
    "    words2ids_filtered[x] = i\n",
    "    vectors_filtered.append(np.random.uniform(-1,1,300))\n",
    "    \n",
    "vectors_filtered.append(np.random.uniform(-1,1,300))\n",
    "vectors_filtered = np.array(vectors_filtered)\n",
    "\n",
    "\n",
    "w2vecs = {}\n",
    "w2vecs[\"words2ids\"] = words2ids_filtered\n",
    "w2vecs[\"vectors\"] = vectors_filtered\n",
    "\n",
    "pickle.dump(w2vecs,open(\"embeddings/filtered_w2v_allwiki_nkjpfull_300.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2vecs[\"words2ids\"]), max(list(w2vecs[\"words2ids\"].values())), w2vecs[\"vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "\n",
    "# to nie bylo zrobione a powinno byc\n",
    "# zostalo to zrobione pozniej i jest juz wszystko ok\n",
    "# chodzi o to, ze do sieci zostaly wczytane tylko embeddingi slow ze zbioru uczacego\n",
    "# i przez to trzeba reczenie dodac embeddingi slow ze zbioru testowego.\n",
    "# oczywiscie trzeba bylo to zrobic od razu za jednym zamachem\n",
    "\n",
    "w2vecs_full = pickle.load(open(\"embeddings/w2v_allwiki_nkjpfull_300.pkl\",\"rb\"),encoding=\"latin1\")\n",
    "\n",
    "datasets = [\"../Składnica_preprocessed_training_data/tokens.txt\",\"../Składnica_preprocessed_test_data/tokens.txt\"]\n",
    "\n",
    "words = set()\n",
    "for dataset in datasets:\n",
    "    with open(dataset) as f:\n",
    "        for x in f.read().split():\n",
    "            words.add(x)\n",
    "\n",
    "words2ids_filtered = {}\n",
    "vectors_filtered = []\n",
    "\n",
    "i = 0\n",
    "for word in w2vecs_full.index2word:\n",
    "    if word in words:\n",
    "        words2ids_filtered[word] = i\n",
    "        vectors_filtered.append(w2vecs_full[word])\n",
    "        i = i + 1        \n",
    "\n",
    "t = open(\"../Składnica_preprocessed_test_data/tokens.txt\",\"r\")\n",
    "tokens.extend([x.split() for x in t.readlines()])\n",
    "t.close()\n",
    "\n",
    "r = open(\"../Składnica_preprocessed_test_data/rules.txt\",\"r\")\n",
    "rules.extend([x.split() for x in r.readlines()])\n",
    "r.close()\n",
    "\n",
    "\n",
    "unique_rules = []\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        if tokens[i][j][:2] == \"__\":\n",
    "            unique_rules.append(tokens[i][j]+rules[i][j])\n",
    "            \n",
    "            \n",
    "for i, x in zip(range(len(vectors_filtered), len(vectors_filtered) + len(unique_rules)), unique_rules):\n",
    "    \n",
    "    words2ids_filtered[x] = i\n",
    "    vectors_filtered.append(np.random.uniform(-0.1,0.1,300))\n",
    "    \n",
    "vectors_filtered.append(np.random.uniform(-0.1,0.1,300))\n",
    "vectors_filtered = np.array(vectors_filtered)\n",
    "\n",
    "\n",
    "w2vecs = {}\n",
    "w2vecs[\"words2ids\"] = words2ids_filtered\n",
    "w2vecs[\"vectors\"] = vectors_filtered\n",
    "\n",
    "pickle.dump(w2vecs,open(\"embeddings/filtered_train_and_test_w2v_allwiki_nkjpfull_300.pkl\",\"wb\"))\n",
    "\n",
    "emb1 = pickle.load(open(\"embeddings/filtered_train_and_test_w2v_allwiki_nkjpfull_300.pkl\",\"rb\"))\n",
    "emb2 = pickle.load(open(\"embeddings/filtered_w2v_allwiki_nkjpfull_300.pkl\",\"rb\"))\n",
    "\n",
    "emb2[\"vectors\"] = np.concatenate([emb2[\"vectors\"], np.zeros((len(emb1[\"words2ids\"])-len(emb2[\"words2ids\"]),300))])\n",
    "j = len(emb2[\"words2ids\"])\n",
    "for word, i in emb1[\"words2ids\"].items():\n",
    "    if word not in emb2[\"words2ids\"].keys():\n",
    "        emb2[\"words2ids\"][word] = j\n",
    "        emb2[\"vectors\"][j] = emb1[\"vectors\"][emb1[\"words2ids\"][word]]\n",
    "        j = j+1\n",
    "        \n",
    "pickle.dump(emb2,open(\"embeddings/filtered_train_and_test_w2v_allwiki_nkjpfull_300.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_from_down_to_top_order(sentence_tree):\n",
    "    #print sentence_tree\n",
    "    levels = np.setdiff1d(range(len(sentence_tree)),np.unique(sentence_tree)) # - zwraca slowo/a, ktore nie jest niczyim dzieckiem - czyli powinno/y byc korzeniem/korzeniami frazy/fraz\n",
    "    if len(levels) == 0: # wczesniej bylo != 1, co oznaczalo, ze jezeli okazuje sie jest wiecej niz jeden korzec (lub nie ma korzenia) to zwracamy None, aby pozniej rozpoznac takie zdanie i je wywalic. Ale jak robimy batche to musi byc kilka korzeni\n",
    "        return None, None\n",
    "    levels = levels.tolist() \n",
    "\n",
    "    for i in range(len(sentence_tree)):\n",
    "        x = np.setdiff1d(sentence_tree[levels[i]],-1)\n",
    "        levels.extend(x[x<len(sentence_tree)])\n",
    "            \n",
    "    ordered_words = np.array(levels)[levels != np.array(-1)][::-1] #odwracamy kolejnosc na poczatku beda slowa znajdujace sie najglebiej\n",
    "    \n",
    "    order = np.zeros(len(sentence_tree),dtype='int')\n",
    "    for i in range(len(sentence_tree)):\n",
    "        order[ordered_words[i]] = i\n",
    "\n",
    "    return ordered_words, order\n",
    "\n",
    "\n",
    "def load_stanford_data4(labels, parents, tokens, rules, words2ids, use_batch, batch_size, nb_classes):\n",
    "\n",
    "\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    l = open(labels, \"r\")\n",
    "    # 5 klas: labels = [[2 if y=='#' else int(y)+2 for y in x.split()] for x in l.readlines()] \n",
    "\n",
    "    # Na ten moment przyjmujemy wartosc \"2\" w miejsce \"#\"\n",
    "\n",
    "    labels = [[int(y) for y in x.split()] for x in l.readlines()] \n",
    "    l.close()\n",
    "\n",
    "    p = open(parents,\"r\")\n",
    "    parents = [[int(y) for y in x.split()] for x in p.readlines()]\n",
    "    p.close()\n",
    "\n",
    "    t = open(tokens,\"r\")\n",
    "    tokens = [x.split() for x in t.readlines()]\n",
    "    t.close()\n",
    "    \n",
    "    r = open(rules,\"r\")\n",
    "    rules = [x.split() for x in r.readlines()]\n",
    "    r.close()\n",
    "    \n",
    "    k = 0\n",
    "    sentence_length = 0\n",
    "    current_batch, batch_tokens, batch_children_ids, batch_children_positions, batch_labels = [], [], [], [], []\n",
    "    batch_words = []\n",
    "    \n",
    "    for labels_i, parents_i, tokens_i, rules_i in zip(labels,parents,tokens,rules):\n",
    "        \n",
    "        k = k + 1\n",
    "         \n",
    "        s = []\n",
    "        for i in range(len(tokens_i)):\n",
    "            \n",
    "            if tokens_i[i] == \"__head_unknown__\":\n",
    "                token = \"__head_unknown__\"+rules_i[i]\n",
    "            else:\n",
    "                token = tokens_i[i]\n",
    "            \n",
    "            \n",
    "            s.append([i,int(parents_i[i]),labels_i[i],token])\n",
    "\n",
    "        if len(s) == 1 and use_batch == False: #przypadek gdy fraza sklada sie z jednego tokena\n",
    "\n",
    "            #if nb_classes == 2:\n",
    "            #    if s[0][-1] < 0:\n",
    "            #        continue\n",
    "\n",
    "            sentences.append([\\\n",
    "                                  np.array([words2ids.get(tokens[0], -1)]),\\\n",
    "                                  #wyrzucamy macierz id dzieci numpy.array([-1], ndmin=2),\\\n",
    "                                  np.array([-1], ndmin=2), \\\n",
    "                                  np.array(labels_i[0]) \\\n",
    "                                  #,numpy.array([0])\n",
    "                              ])    \n",
    "                                \n",
    "        else: \n",
    "\n",
    "            for i in range(len(s)): # nie wiem czy sie nie wywali dla frazy dlugosci 1\n",
    "                children = []\n",
    "                for j in range(len(s)):\n",
    "                    if s[j][1] == i+1:\n",
    "                        children.append(s[j][0])\n",
    "                s[i].append(children)\n",
    "\n",
    "            words = [x[0] for x in s]\n",
    "            children = seq.pad_sequences([x[4] for x in s], padding='post', value = -1)\n",
    "            tokens = [x[3] for x in s]\n",
    "            labels_in_batch = [x[2] for x in s]\n",
    "            \n",
    "            ordered_words, order = words_in_from_down_to_top_order(children)\n",
    "\n",
    "            if ordered_words is None: \n",
    "                continue\n",
    "\n",
    "            current_sentence = [\n",
    "                                  np.array([words2ids.get(x,-1) for x in tokens])[ordered_words],\n",
    "                                  #wyrzucamy macierz id dzieci numpy.array([[words2ids.get(tokens[w],-1) if w>=0 else -1 for w in x] \n",
    "                                  #             for x in children[ordered_words]]), \n",
    "                                  np.array([[order[w] if w>= 0 else -1 for w in x] for x in children[ordered_words]]), \n",
    "                                  np.array(labels_in_batch)[ordered_words] \n",
    "                                  ,np.array(words)\n",
    "                                  ]\n",
    "            #if nb_classes == 2:\n",
    "            #    if current_sentence[3][-1] <0:\n",
    "            #        continue\n",
    "\n",
    "            if use_batch == True:\n",
    "                \n",
    "                # w tej chwili len(current_sentence[0]) nie jest nigdzie wykorzystywane\n",
    "                current_batch.append((current_sentence, len(current_sentence[0])))\n",
    "                \n",
    "                if len(current_batch) % batch_size == 0:\n",
    "\n",
    "                    shift = 0\n",
    "                    \n",
    "                    for sent in range(batch_size):\n",
    "                        \n",
    "                        ##if sent > 0:\n",
    "                        ##    shift = shift + current_batch[sent-1][1]\n",
    "                        \n",
    "                        for tok in range(len(current_batch[sent][0][0])):\n",
    "                            \n",
    "                            if sent == 0:\n",
    "                                batch_children_positions.append(current_batch[sent][0][1][tok])\n",
    "                            else:\n",
    "                                batch_children_positions.append([chd+shift if chd>=0 else -1 for chd in current_batch[sent][0][1][tok]])\n",
    "                            #batch_children_positions.append(current_batch[sent][0][2][tok])\n",
    "\n",
    "                            batch_tokens.append(current_batch[sent][0][0][tok])   \n",
    "                            #wyrzucamy macierz id dzieci batch_children_ids.append(current_batch[sent][0][1][tok])\n",
    "                            batch_labels.append(current_batch[sent][0][2][tok])\n",
    "                            batch_words.append(current_batch[sent][0][3][tok])\n",
    "                                                               \n",
    "                    #wyrzucamy macierz id dzieci batch_children_ids = seq.pad_sequences(batch_children_ids, padding='post', value = -1)\n",
    "                    batch_children_positions = seq.pad_sequences(batch_children_positions, padding='post', value = -1)\n",
    "                    \n",
    "                    sentences.append([\n",
    "                                        np.array(batch_tokens), \n",
    "                                        #wyrzucamy macierz id dzieci numpy.array(batch_children_ids), \n",
    "                                        np.array(batch_children_positions), \n",
    "                                        np.array(batch_labels)\n",
    "                                        ,np.array(batch_words)\n",
    "                                    ])\n",
    "                    \n",
    "                    current_batch, batch_tokens, batch_children_positions, batch_labels = [], [], [], []\n",
    "                    batch_words = []\n",
    "                                       \n",
    "            else:\n",
    "                \n",
    "                sentences.append(current_sentence)\n",
    "\n",
    "            \n",
    "    # gdy liczba zdan nie jest wilokrotnosci licznosci batch, to na koncu trzeba dodac pozostale zdania:\n",
    "    if use_batch == True and len(current_batch) > 0:\n",
    "        \n",
    "        shift = 0\n",
    "\n",
    "        for sent in range(len(current_batch)):\n",
    " \n",
    "            #if sent > 0:\n",
    "            #    shift = shift + current_batch[sent-1][1]\n",
    "\n",
    "            for tok in range(len(current_batch[sent][0][0])):\n",
    "\n",
    "                if sent == 0:\n",
    "                    batch_children_positions.append(current_batch[sent][0][1][tok])\n",
    "                else:\n",
    "                    batch_children_positions.append([chd+shift if chd>=0 else -1 for chd in current_batch[sent][0][1][tok]])\n",
    "                #batch_children_positions.append(current_batch[sent][0][2][tok])\n",
    "\n",
    "                batch_tokens.append(current_batch[sent][0][0][tok])\n",
    "                #wyrzucamy macierz id dzieci batch_children_ids.append(current_batch[sent][0][1][tok])\n",
    "                batch_labels.append(current_batch[sent][0][2][tok])\n",
    "                batch_words.append(current_batch[sent][0][3][tok])\n",
    "\n",
    "\n",
    "        #wyrzucamy macierz id dzieci batch_children_ids = seq.pad_sequences(batch_children_ids, padding='post', value = -1)\n",
    "        batch_children_positions = seq.pad_sequences(batch_children_positions, padding='post', value = -1)\n",
    "\n",
    "        sentences.append([\n",
    "                            np.array(batch_tokens), \n",
    "                            #wyrzucamy macierz id dzieci numpy.array(batch_children_ids), \n",
    "                            np.array(batch_children_positions), \n",
    "                            np.array(batch_labels)\n",
    "                            ,np.array(batch_words)\n",
    "                        ])\n",
    "           \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLSTM(object):  \n",
    "\n",
    "    def __init__(self, h_dim, nc, w2v_model_path, file_with_rules, \n",
    "                 rules_emb_dim, max_phrase_length, emb_dropout_rate, h_dropout_rate, l, srng,\n",
    "                load_params=None): \n",
    "\n",
    "        '''\n",
    "\n",
    "        - dropout stanu ukrytego (LSTM_1)\n",
    "        - dropout embeddinga (LSTM_1)\n",
    "        - regularyzacja l2 (LSTM_1)\n",
    "        - indywidualna obsluga lisci - struktura taka sama, macierze te same, ale uczymy: h_aggregated_0, hidden_state_0, cell_state_0, zamiast brac w te miejsca 0\n",
    "\n",
    "\n",
    "        nh :: dimension of hidden state\n",
    "        nc :: number of classes\n",
    "        '''\n",
    "\n",
    "        self.max_phrase_length = max_phrase_length\n",
    "\n",
    "        w2vecs = pickle.load(open(w2v_model_path,\"rb\"))\n",
    "        self.emb = theano.shared(w2vecs[\"vectors\"].astype(theano.config.floatX))\n",
    "        self.words2ids = w2vecs[\"words2ids\"]\n",
    "\n",
    "        emb_dim = w2vecs[\"vectors\"].shape[1]\n",
    "        del w2vecs\n",
    "\n",
    "        \n",
    "        r = open(file_with_rules,\"r\")\n",
    "        rules = [x.split() for x in r.readlines()]\n",
    "        r.close()\n",
    "        unique_rules = set()\n",
    "        for i in range(len(rules)):\n",
    "            for j in range(len(rules[i])):\n",
    "                unique_rules.add(rules[i][j])\n",
    "                \n",
    "        number_of_uniue_rules = len(unique_rules)\n",
    " \n",
    "        r = 0.05\n",
    "\n",
    "        self.rules2ids = dict(zip(unique_rules,range(number_of_uniue_rules)))\n",
    "        self.emb_rules = theano.shared(r * np.random.uniform(-1,1,(number_of_uniue_rules, rules_emb_dim)).astype(theano.config.floatX))\n",
    "        \n",
    "   \n",
    "\n",
    "        self.W_i = theano.shared(r * np.random.uniform(-1.0, 1.0, (emb_dim+rules_emb_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.U_i = theano.shared(r * np.random.uniform(-1.0, 1.0, (h_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.b_i = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "\n",
    "        self.W_f = theano.shared(r * np.random.uniform(-1.0, 1.0, (emb_dim+rules_emb_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.U_f = theano.shared(r * np.random.uniform(-1.0, 1.0, (h_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.b_f = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "\n",
    "        self.W_o = theano.shared(r * np.random.uniform(-1.0, 1.0, (emb_dim+rules_emb_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.U_o = theano.shared(r * np.random.uniform(-1.0, 1.0, (h_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.b_o = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "\n",
    "        self.W_u = theano.shared(r * np.random.uniform(-1.0, 1.0, (emb_dim+rules_emb_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.U_u = theano.shared(r * np.random.uniform(-1.0, 1.0, (h_dim, h_dim) ).astype(theano.config.floatX))\n",
    "        self.b_u = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "\n",
    "        self.W_y   = theano.shared(r * np.random.uniform(-1.0, 1.0, (h_dim, nc)).astype(theano.config.floatX))\n",
    "        self.b_y   = theano.shared(r * np.random.uniform(-1.0, 1.0, nc).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.h_aggregated_0 = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "        self.cell_state_0 = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "        self.hidden_state_0 = theano.shared(r * np.random.uniform(-1.0, 1.0, h_dim ).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "\n",
    "        self.srng = srng\n",
    "        self.h_dropout_rate = h_dropout_rate\n",
    "        self.emb_dropout_rate = emb_dropout_rate\n",
    "        self.l = l\n",
    "\n",
    "\n",
    "        if load_params:\n",
    "            load_params = pickle.load(open(load_params,\"rb\"))\n",
    "            if type(load_params)==list:\n",
    "                load_params = dict(load_params)\n",
    "            for key in load_params.keys():\n",
    "                if key not in ['emb', 'emb_rules', 'W_i', 'U_i', 'b_i', 'W_f', 'U_f', 'b_f', 'W_o', 'U_o', 'b_o', 'W_u', 'U_u', 'b_u', 'W_y', 'b_y', 'h_aggregated_0', 'cell_state_0', 'hidden_state_0']:\n",
    "                    setattr(self, key, load_params[key])\n",
    "                else:\n",
    "                    setattr(self, key, theano.shared(load_params[key]))\n",
    "        \n",
    "        \n",
    "\n",
    "        def one_step(word_id, rule_id, word_children_positions, y_true, k, hidden_states, cell_states, learning_rate):\n",
    "\n",
    "            x = T.concatenate( [self.emb[word_id], self.emb_rules[rule_id] ])\n",
    "\n",
    "            #dropout:\n",
    "            mask1 = self.srng.binomial(n=1, p=1-self.emb_dropout_rate, size=(emb_dim+rules_emb_dim,), dtype='floatX')\n",
    "            x = x * mask1\n",
    "\n",
    "\n",
    "            tmp = word_children_positions>=0.0\n",
    "            number_of_children = tmp.sum(dtype = theano.config.floatX) \n",
    "            idx_tmp = tmp.nonzero()                                                                   # indeksy realne dzieci - czyli te, gdzie nie ma -1        \n",
    "\n",
    "            h_aggregated = ifelse(T.gt(number_of_children, 0.0), hidden_states[word_children_positions[idx_tmp]].sum(axis=0), self.h_aggregated_0)\n",
    "\n",
    "\n",
    "            i = T.nnet.sigmoid(\tT.dot(x, self.W_i) + T.dot(h_aggregated, self.U_i) + self.b_i)             \n",
    "\n",
    "            o = T.nnet.sigmoid(\tT.dot(x, self.W_o) + T.dot(h_aggregated, self.U_o) + self.b_o)             \n",
    "\n",
    "            u = T.tanh(\tT.dot(x, self.W_u) + T.dot(h_aggregated, self.U_u) + self.b_u)             \n",
    "\n",
    "            f_c = ifelse(T.gt(number_of_children, 0.0), \n",
    "                (T.nnet.sigmoid( T.dot(x, self.W_f ) + T.dot(hidden_states[word_children_positions[idx_tmp]], self.U_f)  + self.b_f )*cell_states[word_children_positions[idx_tmp]]).sum(axis=0),\n",
    "                T.nnet.sigmoid( T.dot(x, self.W_f ) + T.dot(self.hidden_state_0, self.U_f)  + self.b_f ) * self.cell_state_0\n",
    "            )\n",
    "\n",
    "            c = i*u + f_c\n",
    "\n",
    "            h = o * T.tanh(c)\n",
    "            #dropout:\n",
    "            mask = self.srng.binomial(n=1, p=1-self.h_dropout_rate, size=(h_dim,), dtype='floatX')\n",
    "            h = h * mask\n",
    "\n",
    "\n",
    "            current_cell_state = cell_states[k]\n",
    "            cell_states_new = T.set_subtensor(current_cell_state, c)\n",
    "\n",
    "            current_hidden_state = hidden_states[k]\n",
    "            hidden_states_new = T.set_subtensor(current_hidden_state, h)\n",
    "\n",
    "\n",
    "            y_prob = T.nnet.softmax(T.dot(h,self.W_y) + self.b_y)[0]\n",
    "\n",
    "            cross_entropy = -T.log(y_prob[y_true])\t\t\t\t\t\t      \n",
    "\n",
    "            return cross_entropy, hidden_states_new, cell_states_new  \n",
    "\n",
    "\n",
    "        y = T.vector('y',dtype=dataType)\n",
    "        learning_rate = T.scalar('lr',dtype=theano.config.floatX)\n",
    "        words = T.vector(dtype=dataType)\n",
    "        rules = T.vector(dtype=dataType)\n",
    "        children_positions = T.matrix(dtype=dataType)\n",
    "        words_indexes = T.vector(dtype=dataType)\n",
    "\n",
    "        [cross_entropy_vector, _, _] , _ = theano.scan(fn=one_step, \\\n",
    "                                 sequences = [words, rules, children_positions,y,words_indexes],\n",
    "                                 outputs_info = [None, \n",
    "                                     theano.shared(np.zeros((self.max_phrase_length+1,h_dim), dtype = theano.config.floatX)),\n",
    "                                     theano.shared(np.zeros((self.max_phrase_length+1,h_dim), dtype = theano.config.floatX))],\n",
    "                                 non_sequences = learning_rate,\n",
    "                                 n_steps = words.shape[0])\n",
    "\n",
    "        cost = T.mean(cross_entropy_vector) + self.l * (self.emb_rules**2).sum() #*0.5 * self.l * ((self.W_i**2).sum()+(self.W_f**2).sum()+(self.W_o**2).sum()+(self.W_u**2).sum()+(self.W_y**2).sum()+(self.U_i**2).sum()+(self.U_f**2).sum()+(self.U_o**2).sum()+(self.U_u**2).sum())\n",
    "\n",
    "        updates = OrderedDict([\n",
    "            (self.W_i, self.W_i-learning_rate*T.grad(cost, self.W_i)),\n",
    "            (self.W_f, self.W_f-learning_rate*T.grad(cost, self.W_f)),\n",
    "            (self.W_o, self.W_o-learning_rate*T.grad(cost, self.W_o)),\n",
    "            (self.W_u, self.W_u-learning_rate*T.grad(cost, self.W_u)),\n",
    "            (self.W_y, self.W_y-learning_rate*T.grad(cost, self.W_y)),\n",
    "\n",
    "            (self.U_i, self.U_i-learning_rate*T.grad(cost, self.U_i)),\n",
    "            (self.U_f, self.U_f-learning_rate*T.grad(cost, self.U_f)),\n",
    "            (self.U_o, self.U_o-learning_rate*T.grad(cost, self.U_o)),\n",
    "            (self.U_u, self.U_u-learning_rate*T.grad(cost, self.U_u)),\n",
    "\n",
    "            #(self.emb, self.emb-learning_rate*T.grad(cost, self.emb)), #SPROBOWAC TU 0.1 ZAMIAST LR, A DLA POLSKICH BEZ AKTUALIZACJI EMB\n",
    "            (self.emb_rules, self.emb_rules-learning_rate*T.grad(cost, self.emb_rules)),\n",
    "            (self.b_i, self.b_i-learning_rate*T.grad(cost,self.b_i)),\n",
    "                        (self.b_f, self.b_f-learning_rate*T.grad(cost,self.b_f)),\n",
    "                        (self.b_o, self.b_o-learning_rate*T.grad(cost,self.b_o)),\n",
    "                        (self.b_u, self.b_u-learning_rate*T.grad(cost,self.b_u)),\n",
    "                        (self.b_y, self.b_y-learning_rate*T.grad(cost,self.b_y)),\n",
    "\n",
    "            (self.h_aggregated_0, self.h_aggregated_0-learning_rate*T.grad(cost,self.h_aggregated_0)),\n",
    "            (self.cell_state_0, self.cell_state_0-learning_rate*T.grad(cost,self.cell_state_0)),\n",
    "            (self.hidden_state_0, self.hidden_state_0-learning_rate*T.grad(cost,self.hidden_state_0))\n",
    "\n",
    "            ])\n",
    "\n",
    "        self.train = theano.function( inputs  = [words, rules, children_positions, y, words_indexes, learning_rate],\n",
    "                                      outputs = [],\n",
    "                                      updates = updates,\n",
    "                                      allow_input_downcast=True,\n",
    "                                      mode='FAST_RUN'\n",
    "                                      )\n",
    "\n",
    "\n",
    "        def one_step_classify(word_id, rule_id, word_children_positions, k, hidden_states, cell_states):\n",
    "\n",
    "            x = T.concatenate( [self.emb[word_id], self.emb_rules[rule_id] ])\n",
    "\n",
    "            x = (1-self.emb_dropout_rate) * x\n",
    "\n",
    "            tmp = word_children_positions>=0.0\n",
    "            number_of_children = tmp.sum(dtype = theano.config.floatX) \n",
    "            idx_tmp = tmp.nonzero()                                                                   # indeksy realne dzieci - czyli te, gdzie nie ma -1        \n",
    "\n",
    "            h_aggregated = ifelse(T.gt(number_of_children, 0.0), hidden_states[word_children_positions[idx_tmp]].sum(axis=0), self.h_aggregated_0)\n",
    "\n",
    "            i = T.nnet.sigmoid(\tT.dot(x, self.W_i) + T.dot(h_aggregated, self.U_i) + self.b_i)             \n",
    "\n",
    "            o = T.nnet.sigmoid(\tT.dot(x, self.W_o) + T.dot(h_aggregated, self.U_o) + self.b_o)             \n",
    "\n",
    "            u = T.tanh(\tT.dot(x, self.W_u) + T.dot(h_aggregated, self.U_u) + self.b_u)             \n",
    "\n",
    "            f_c = ifelse(T.gt(number_of_children, 0.0), \n",
    "                (T.nnet.sigmoid( T.dot(x, self.W_f ) + T.dot(hidden_states[word_children_positions[idx_tmp]], self.U_f)  + self.b_f )*cell_states[word_children_positions[idx_tmp]]).sum(axis=0),\n",
    "                T.nnet.sigmoid( T.dot(x, self.W_f ) + T.dot(self.hidden_state_0, self.U_f)  + self.b_f ) * self.cell_state_0\n",
    "            )\n",
    "\n",
    "            c = i*u + f_c\n",
    "\n",
    "            h = o * T.tanh(c)\n",
    "            # podczas uczenia zerowalismy 1-dropout_rate procent wspolrzednych, wiec trzeba to \n",
    "            h = h * (1-self.h_dropout_rate)\n",
    "\n",
    "            current_cell_state = cell_states[k]\n",
    "            cell_states_new = T.set_subtensor(current_cell_state, c)\n",
    "\n",
    "            current_hidden_state = hidden_states[k]\n",
    "            hidden_states_new = T.set_subtensor(current_hidden_state, h)\n",
    "\n",
    "\n",
    "            y_prob = T.nnet.softmax(T.dot(h,self.W_y) + self.b_y)[0]             \n",
    "\n",
    "            return  y_prob, hidden_states_new, cell_states_new\n",
    "\n",
    "\n",
    "        [y_probs_classify, _ , _ ], _ = theano.scan(\n",
    "                 fn=one_step_classify, \n",
    "                                 sequences = [words, rules, children_positions, words_indexes],\n",
    "                 outputs_info = [None,\n",
    "                         theano.shared(np.zeros((self.max_phrase_length+1,h_dim), dtype = theano.config.floatX)),\n",
    "                         theano.shared(np.zeros((self.max_phrase_length+1,h_dim), dtype = theano.config.floatX))])\n",
    "\n",
    "        predictions, _ = theano.scan(lambda i: T.argmax(y_probs_classify[i]), \n",
    "                                     sequences = [words_indexes])\n",
    "        \n",
    "        probs, _ = theano.scan(lambda i: y_probs_classify[i], \n",
    "                                     sequences = [words_indexes])\n",
    "\n",
    "        self.classify = theano.function(inputs=[words, rules, children_positions,words_indexes], \n",
    "                                     outputs=predictions,\n",
    "                                     allow_input_downcast=True,\n",
    "                                     mode='FAST_RUN' \n",
    "                                     )\n",
    "\n",
    "        self.predict_proba = theano.function(inputs=[words, rules, children_positions,words_indexes], \n",
    "                             outputs=probs,\n",
    "                             allow_input_downcast=True,\n",
    "                             mode='FAST_RUN' \n",
    "                             )\n",
    "\n",
    "        self.calculate_loss = theano.function(inputs=[words, rules, children_positions, y, words_indexes, learning_rate], \n",
    "                     outputs=cost,\n",
    "                     allow_input_downcast=True,\n",
    "                     mode='FAST_RUN' \n",
    "                     )\n",
    "        \n",
    "    def save_model(self,path):\n",
    "        params = [ (k, v.get_value())  if type(v)==theano.tensor.sharedvar.TensorSharedVariable else (k,v) for k, v in list(self.__dict__.items())]\n",
    "        params = dict(params)\n",
    "        pickle.dump(params,open(path,\"wb\"))\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import  csv\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from keras.preprocessing import sequence as seq\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.ifelse import ifelse\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams \n",
    "\n",
    "\n",
    "\n",
    "s = {'lr':0.05,\n",
    "         'nepochs':80,\n",
    "         'seed':345,\n",
    "         'nc':2,        # number of y classes\n",
    "         'h_dim': 100,\n",
    "         'h_dropout_rate': 0.5,\n",
    "         'emb_dropout_rate': 0.5,\n",
    "         'time_without_improvement': 10,\n",
    "         'batch_size': 1,\n",
    "         'w2v_DIM': \"300\",\n",
    "         \"rules_emb_dim\": 30\n",
    "         }  \n",
    "\n",
    "dataType = 'int64'\n",
    "  \n",
    "np.random.seed(s['seed']) \n",
    "\n",
    "#ile_with_filtered_embeddings = \"embeddings/filtered_nkjp+wiki-forms-restricted-300-cbow-ns.pkl\"\n",
    "#2vecs = pickle.load(open(file_with_filtered_embeddings,\"rb\"))\n",
    "\n",
    "rnn = TreeLSTM( h_dim = s['h_dim'],\n",
    "            nc = s['nc'],\n",
    "        w2v_model_path = \"embeddings/filtered_w2v_allwiki_nkjpfull_300.pkl\",\n",
    "            max_phrase_length = 100,\n",
    "        emb_dropout_rate = s['emb_dropout_rate'],\n",
    "        h_dropout_rate = s['h_dropout_rate'],\n",
    "        l = 0.0001,\n",
    "        srng = RandomStreams(12345),\n",
    "        file_with_rules =  \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/rules.txt\",\n",
    "        rules_emb_dim = s[\"rules_emb_dim\"],\n",
    "        load_params= \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_116.pkl\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:134: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:227: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0 = load_stanford_data4(\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/labels.txt\", \n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/parents.txt\",\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/tokens.txt\",\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/rules.txt\",\n",
    "    rnn.words2ids,True,s['batch_size'],s['nc'])\n",
    "\n",
    "data_rules = load_stanford_data4(\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/labels.txt\", \n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/parents.txt\",\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/rules.txt\",\n",
    "    \"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Składnica_preprocessed_training_data/rules.txt\",\n",
    "    rnn.rules2ids,True,s['batch_size'],s['nc'])\n",
    "\n",
    "data_rules = [x[0] for x in data_rules]\n",
    "\n",
    "data = [data0[i]+[data_rules[i]] for i in range(len(data0))]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 10495)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_ind = np.random.choice(range(len(data)), 80000, replace = False)\n",
    "#validation_ind = np.setdiff1d(range(len(data)),train_ind)\n",
    "\n",
    "#pickle.dump(train_ind,open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/train_observations\",\"wb\"))\n",
    "#pickle.dump(validation_ind,open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/validation_observations\",\"wb\"))\n",
    "\n",
    "train_ind = pickle.load(open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/train_observations\",\"rb\"))\n",
    "validation_ind = pickle.load(open(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/validation_observations\",\"rb\"))\n",
    "\n",
    "train_data = [data[i] for i in train_ind]\n",
    "validation_data = [data[i] for i in validation_ind]\n",
    "\n",
    "n_train = len(train_data)\n",
    "n_val = len(validation_data)\n",
    "\n",
    "n_train, n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([11475,  1727,  1333,     0,   838,   838,   443,  1555,  3073,\n",
       "           25,    25,    25,  6160,  6160,  4735,  6160,   229,     0,\n",
       "        19049, 19049, 19049, 19049,    -1, 19049]), array([[-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [ 1,  0, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [ 3,  2, -1, -1],\n",
       "        [ 5,  4, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [ 8,  7,  6, -1],\n",
       "        [11, 10,  9, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [15, 14, 13, 12],\n",
       "        [17, 16, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [20, 19, 18, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [22, 21, -1, -1]], dtype=int32), array([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23]), array([ 80,  72,  80,  48,  20,  48,  80,  80,  72, 116, 116,  10,  20,\n",
       "        116,  56,  80,  37,  48, 108,  87,  59, 105,  96,   7])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_trees = np.random.choice(range(len(train_data)), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  572,    20,  1225, 22175,  7713, 25375, 10157,    11,    11,\n",
       "        14568, 14568, 14568, 14568,  3807,  3099, 27615, 14568,    -1,\n",
       "         8608,  8608,  8608,  8608,    -1,  8608]),\n",
       " array([[-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [ 2,  1,  0, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [ 4,  3, -1, -1, -1],\n",
       "        [ 6,  5, -1, -1, -1],\n",
       "        [ 8,  7, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [13, 12, 11, 10,  9],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [15, 14, -1, -1, -1],\n",
       "        [17, 16, -1, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [20, 19, 18, -1, -1],\n",
       "        [-1, -1, -1, -1, -1],\n",
       "        [22, 21, -1, -1, -1]], dtype=int32),\n",
       " array([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23]),\n",
       " array([ 80,  72,  80,  56,  80,  80,  80,  20,  48, 108, 108,  87,  59,\n",
       "          6,  80,  80, 105, 101, 108,  89,  59, 105,  96,   7])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "13  Train:  82.76  Train root:  90.10  Valid all:  82.36  Valid root:  89.08  Valid cost:  0.3847145544493721    time:  3013.6113007068634\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "14  Train:  83.39  Train root:  91.17  Valid all:  82.77  Valid root:  89.58  Valid cost:  0.37616646006391014    time:  3114.761006593704\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "15  Train:  83.19  Train root:  90.74  Valid all:  82.65  Valid root:  89.15  Valid cost:  0.37880950172730377    time:  3249.0635669231415\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "16  Train:  83.42  Train root:  90.70  Valid all:  82.88  Valid root:  89.89  Valid cost:  0.3749994219349553    time:  3435.1438908576965\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "17  Train:  82.85  Train root:  90.84  Valid all:  82.54  Valid root:  89.84  Valid cost:  0.37964782381076195    time:  3276.254175901413\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "18  Train:  83.57  Train root:  91.28  Valid all:  83.02  Valid root:  89.84  Valid cost:  0.3731835962437792    time:  3272.7676355838776\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "19  Train:  83.32  Train root:  91.06  Valid all:  82.85  Valid root:  89.44  Valid cost:  0.37495716684588853    time:  3268.551740169525\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "20  Train:  83.36  Train root:  91.18  Valid all:  82.87  Valid root:  89.60  Valid cost:  0.37523761880488665    time:  3273.650741815567\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "21  Train:  83.57  Train root:  91.43  Valid all:  83.05  Valid root:  89.45  Valid cost:  0.37342573217080355    time:  3283.2291388511658\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "22  Train:  83.61  Train root:  91.68  Valid all:  83.20  Valid root:  90.35  Valid cost:  0.3707790091218495    time:  3255.7854731082916\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "23  Train:  83.46  Train root:  91.80  Valid all:  82.88  Valid root:  90.18  Valid cost:  0.3755694362682323    time:  3304.151799917221\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "24  Train:  83.56  Train root:  91.26  Valid all:  83.18  Valid root:  90.08  Valid cost:  0.37161634617430533    time:  3243.6698100566864\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "25  Train:  83.76  Train root:  91.97  Valid all:  83.24  Valid root:  90.15  Valid cost:  0.3688804581700573    time:  3346.2038316726685\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "26  Train:  83.67  Train root:  91.69  Valid all:  83.26  Valid root:  90.51  Valid cost:  0.36817428519405343    time:  3152.8867497444153\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "27  Train:  83.99  Train root:  92.05  Valid all:  83.32  Valid root:  90.34  Valid cost:  0.3684120555020063    time:  3308.534412384033\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "28  Train:  83.85  Train root:  91.49  Valid all:  83.29  Valid root:  90.30  Valid cost:  0.36894053177701297    time:  4316.936202287674\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "29  Train:  83.86  Train root:  91.69  Valid all:  83.28  Valid root:  90.30  Valid cost:  0.36940085959894536    time:  3221.766302585602\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "30  Train:  83.95  Train root:  91.78  Valid all:  83.36  Valid root:  90.34  Valid cost:  0.3673957693145183    time:  2970.8190462589264\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "31  Train:  83.79  Train root:  91.89  Valid all:  83.42  Valid root:  90.51  Valid cost:  0.3678159104983166    time:  2953.8967123031616\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "32  Train:  83.61  Train root:  91.64  Valid all:  83.15  Valid root:  90.43  Valid cost:  0.3686115632822676    time:  2954.4148285388947\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "33  Train:  84.01  Train root:  92.46  Valid all:  83.32  Valid root:  90.53  Valid cost:  0.36767972222276    time:  2972.4844381809235\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "34  Train:  84.09  Train root:  92.14  Valid all:  83.31  Valid root:  90.48  Valid cost:  0.36605065249628954    time:  2964.4687383174896\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "35  Train:  83.58  Train root:  92.25  Valid all:  83.04  Valid root:  90.52  Valid cost:  0.3722481994900325    time:  2967.1328225135803\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "36  Train:  83.72  Train root:  91.93  Valid all:  83.37  Valid root:  90.72  Valid cost:  0.366441007728656    time:  2979.168189048767\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "37  Train:  84.04  Train root:  92.70  Valid all:  83.45  Valid root:  90.67  Valid cost:  0.36538400138981675    time:  2976.5577709674835\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "38  Train:  84.11  Train root:  91.85  Valid all:  83.42  Valid root:  90.51  Valid cost:  0.36607755430529926    time:  2984.145543575287\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "39  Train:  84.08  Train root:  91.90  Valid all:  83.36  Valid root:  90.66  Valid cost:  0.36600933056298096    time:  3042.4532086849213\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "40  Train:  83.93  Train root:  92.22  Valid all:  83.27  Valid root:  90.40  Valid cost:  0.3705316014106023    time:  3311.2729094028473\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "41  Train:  84.25  Train root:  92.11  Valid all:  83.48  Valid root:  90.89  Valid cost:  0.3651340626606566    time:  3091.0792996883392\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "42  Train:  84.13  Train root:  92.83  Valid all:  83.51  Valid root:  90.64  Valid cost:  0.36575582693093567    time:  3338.264650821686\n",
      "0\n",
      "10000\n",
      "20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5e29ffd34a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/norbert/.theano/compiledir_Linux-4.13--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.3-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_train_trees = np.random.choice(range(len(train_data)), 10000)\n",
    "\n",
    "for e in range(13,100):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], s['lr'])\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "42  Train:  83.97  Train root:  92.40  Valid all:  83.30  Valid root:  90.68  Valid cost:  0.36612108856680964    time:  3252.7360546588898\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "43  Train:  84.02  Train root:  92.43  Valid all:  83.41  Valid root:  90.83  Valid cost:  0.3635446132947844    time:  3066.4460577964783\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "44  Train:  83.70  Train root:  91.85  Valid all:  83.33  Valid root:  90.64  Valid cost:  0.3646511884730552    time:  3424.4148004055023\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "45  Train:  83.94  Train root:  91.83  Valid all:  83.46  Valid root:  90.82  Valid cost:  0.36554392016851733    time:  3288.298787355423\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "46  Train:  84.03  Train root:  92.52  Valid all:  83.46  Valid root:  90.97  Valid cost:  0.36326602926799917    time:  3211.7949781417847\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "47  Train:  83.81  Train root:  92.36  Valid all:  83.34  Valid root:  90.70  Valid cost:  0.36493544259834937    time:  3220.5043840408325\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "48  Train:  84.10  Train root:  91.88  Valid all:  83.46  Valid root:  90.85  Valid cost:  0.36396660424632266    time:  3215.0404415130615\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "49  Train:  84.12  Train root:  91.82  Valid all:  83.47  Valid root:  90.39  Valid cost:  0.3666669648269341    time:  3212.0493834018707\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "50  Train:  84.35  Train root:  92.74  Valid all:  83.56  Valid root:  91.01  Valid cost:  0.3636573189329542    time:  3244.2998707294464\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "51  Train:  84.22  Train root:  92.76  Valid all:  83.52  Valid root:  90.98  Valid cost:  0.3648860093343714    time:  3218.9965472221375\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "52  Train:  84.06  Train root:  92.36  Valid all:  83.51  Valid root:  90.93  Valid cost:  0.36332647312239963    time:  3245.7509117126465\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "53  Train:  84.21  Train root:  92.42  Valid all:  83.54  Valid root:  91.05  Valid cost:  0.36434035460300934    time:  3223.653787136078\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "54  Train:  84.11  Train root:  93.09  Valid all:  83.52  Valid root:  90.95  Valid cost:  0.362709172839266    time:  3232.888058900833\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "55  Train:  84.43  Train root:  92.83  Valid all:  83.67  Valid root:  90.73  Valid cost:  0.36187091216248857    time:  3232.5271410942078\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "56  Train:  83.62  Train root:  92.28  Valid all:  83.10  Valid root:  90.61  Valid cost:  0.3665147075130506    time:  3276.360540151596\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "57  Train:  84.01  Train root:  92.10  Valid all:  83.35  Valid root:  90.81  Valid cost:  0.3648049766538561    time:  3251.9386920928955\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "58  Train:  84.46  Train root:  92.86  Valid all:  83.66  Valid root:  91.01  Valid cost:  0.36246949101240505    time:  3264.1991336345673\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "59  Train:  84.13  Train root:  92.46  Valid all:  83.52  Valid root:  91.17  Valid cost:  0.36335383456771847    time:  3228.8368022441864\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "60  Train:  84.33  Train root:  92.26  Valid all:  83.74  Valid root:  91.16  Valid cost:  0.36268241882759933    time:  3247.543758392334\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "61  Train:  84.63  Train root:  92.97  Valid all:  83.66  Valid root:  90.92  Valid cost:  0.3591708180395662    time:  3257.489506959915\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "62  Train:  84.48  Train root:  93.44  Valid all:  83.71  Valid root:  90.96  Valid cost:  0.3608405524999677    time:  3343.8545200824738\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "63  Train:  83.94  Train root:  92.72  Valid all:  83.38  Valid root:  91.11  Valid cost:  0.364479419544096    time:  3306.1144137382507\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "64  Train:  84.26  Train root:  92.63  Valid all:  83.67  Valid root:  91.11  Valid cost:  0.36081773826627045    time:  3329.708119869232\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "65  Train:  84.57  Train root:  93.17  Valid all:  83.75  Valid root:  91.34  Valid cost:  0.3598929451502107    time:  3189.976569414139\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "66  Train:  84.58  Train root:  92.73  Valid all:  83.81  Valid root:  91.15  Valid cost:  0.36184262243579146    time:  2952.796290397644\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "67  Train:  84.55  Train root:  92.62  Valid all:  83.73  Valid root:  91.01  Valid cost:  0.3611202626145237    time:  2959.864458322525\n",
      "0\n",
      "10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-777a8844f41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/norbert/.theano/compiledir_Linux-4.13--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.3-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_train_trees = np.random.choice(range(len(train_data)), 10000)\n",
    "\n",
    "for e in range(42,100):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], s['lr'])\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "68  Train:  84.32  Train root:  92.86  Valid all:  83.80  Valid root:  91.21  Valid cost:  0.3576795492335451    time:  3066.740645647049\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "69  Train:  84.78  Train root:  92.79  Valid all:  83.96  Valid root:  91.17  Valid cost:  0.3567972969011771    time:  3028.196727991104\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "70  Train:  84.73  Train root:  92.83  Valid all:  83.94  Valid root:  91.44  Valid cost:  0.3579135542632963    time:  2972.9641377925873\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "71  Train:  84.43  Train root:  92.69  Valid all:  83.76  Valid root:  91.30  Valid cost:  0.3596135877746345    time:  3059.8343963623047\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "72  Train:  84.52  Train root:  92.80  Valid all:  83.87  Valid root:  91.36  Valid cost:  0.3574457594407381    time:  2985.555864572525\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "73  Train:  84.78  Train root:  93.08  Valid all:  83.96  Valid root:  91.23  Valid cost:  0.35515702717559056    time:  2996.4280948638916\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "74  Train:  84.83  Train root:  93.47  Valid all:  83.99  Valid root:  91.33  Valid cost:  0.35550231201731275    time:  3004.8402910232544\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "75  Train:  84.68  Train root:  92.95  Valid all:  83.88  Valid root:  91.34  Valid cost:  0.3548433591768309    time:  3008.333847284317\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "76  Train:  84.70  Train root:  93.25  Valid all:  83.94  Valid root:  91.41  Valid cost:  0.35621494116135594    time:  3015.3657307624817\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "77  Train:  84.79  Train root:  93.69  Valid all:  84.00  Valid root:  91.41  Valid cost:  0.3552697919794545    time:  3123.782624721527\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "78  Train:  84.80  Train root:  92.74  Valid all:  83.97  Valid root:  91.33  Valid cost:  0.3567149873573569    time:  2983.718104839325\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "79  Train:  84.80  Train root:  93.22  Valid all:  83.97  Valid root:  91.31  Valid cost:  0.3543977262754403    time:  2986.0626764297485\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80  Train:  84.74  Train root:  93.19  Valid all:  83.98  Valid root:  91.50  Valid cost:  0.3546578851497851    time:  4133.576906204224\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "81  Train:  84.78  Train root:  93.56  Valid all:  84.00  Valid root:  91.52  Valid cost:  0.3550973487329297    time:  2973.625469684601\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "82  Train:  84.79  Train root:  93.08  Valid all:  84.02  Valid root:  91.39  Valid cost:  0.3551805820453623    time:  2974.987590074539\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "83  Train:  84.64  Train root:  92.65  Valid all:  83.96  Valid root:  91.47  Valid cost:  0.35356023720691443    time:  2970.636495113373\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "84  Train:  84.61  Train root:  93.06  Valid all:  84.01  Valid root:  91.39  Valid cost:  0.3537796645668407    time:  2968.6200728416443\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "85  Train:  84.80  Train root:  93.45  Valid all:  83.98  Valid root:  91.47  Valid cost:  0.3562082436889687    time:  2975.4993345737457\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "86  Train:  84.80  Train root:  93.34  Valid all:  83.99  Valid root:  91.29  Valid cost:  0.35488229799153137    time:  2972.2225334644318\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "87  Train:  85.01  Train root:  93.38  Valid all:  84.04  Valid root:  91.46  Valid cost:  0.3538004218573625    time:  2972.5728595256805\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "88  Train:  84.78  Train root:  92.85  Valid all:  84.01  Valid root:  91.41  Valid cost:  0.355020085068818    time:  2973.248968601227\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "89  Train:  84.97  Train root:  92.76  Valid all:  84.00  Valid root:  91.52  Valid cost:  0.3533910875877329    time:  3112.2544827461243\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "90  Train:  84.61  Train root:  93.30  Valid all:  83.98  Valid root:  91.52  Valid cost:  0.354244535317065    time:  2976.8393416404724\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "91  Train:  84.67  Train root:  93.07  Valid all:  84.01  Valid root:  91.42  Valid cost:  0.35457990064665945    time:  2976.4907603263855\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "92  Train:  84.80  Train root:  93.07  Valid all:  84.05  Valid root:  91.44  Valid cost:  0.35365950653268263    time:  2996.443143606186\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "93  Train:  84.72  Train root:  93.31  Valid all:  84.00  Valid root:  91.60  Valid cost:  0.3531166466543555    time:  2996.235629081726\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "94  Train:  84.81  Train root:  93.30  Valid all:  83.99  Valid root:  91.52  Valid cost:  0.35429932453610596    time:  3002.701921224594\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "95  Train:  84.79  Train root:  92.99  Valid all:  84.03  Valid root:  91.41  Valid cost:  0.3523322058531885    time:  3016.6209206581116\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "96  Train:  84.82  Train root:  93.35  Valid all:  84.05  Valid root:  91.47  Valid cost:  0.3545990467235702    time:  3023.3554747104645\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "97  Train:  84.43  Train root:  93.38  Valid all:  83.97  Valid root:  91.45  Valid cost:  0.35489339688353566    time:  3027.359330177307\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "98  Train:  85.08  Train root:  93.53  Valid all:  84.02  Valid root:  91.34  Valid cost:  0.3528400761394337    time:  3039.2786693573\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "99  Train:  84.72  Train root:  93.10  Valid all:  84.01  Valid root:  91.48  Valid cost:  0.35326843064268604    time:  3038.599848985672\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "100  Train:  84.71  Train root:  93.31  Valid all:  84.00  Valid root:  91.35  Valid cost:  0.35306341595554624    time:  3050.2106494903564\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "101  Train:  84.73  Train root:  93.46  Valid all:  83.98  Valid root:  91.27  Valid cost:  0.35272105080470456    time:  3051.3167984485626\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "102  Train:  84.72  Train root:  93.42  Valid all:  84.03  Valid root:  91.56  Valid cost:  0.3524985972173395    time:  3062.761401414871\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "103  Train:  84.84  Train root:  93.47  Valid all:  84.13  Valid root:  91.39  Valid cost:  0.35444410893029615    time:  3142.613285303116\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "104  Train:  84.67  Train root:  93.05  Valid all:  83.92  Valid root:  91.65  Valid cost:  0.3536345053915415    time:  3263.7068235874176\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "105  Train:  84.90  Train root:  93.67  Valid all:  83.98  Valid root:  91.52  Valid cost:  0.35232265161744375    time:  3168.2654283046722\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "106  Train:  84.83  Train root:  93.41  Valid all:  84.07  Valid root:  91.45  Valid cost:  0.35209813558892755    time:  3192.024574279785\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "107  Train:  84.77  Train root:  93.36  Valid all:  84.09  Valid root:  91.30  Valid cost:  0.35317325968261615    time:  3180.1334657669067\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "108  Train:  84.65  Train root:  93.16  Valid all:  83.86  Valid root:  91.51  Valid cost:  0.35476755034812985    time:  3158.0717403888702\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "109  Train:  84.87  Train root:  93.15  Valid all:  84.03  Valid root:  91.57  Valid cost:  0.35304185655686815    time:  2993.4664890766144\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "110  Train:  84.87  Train root:  93.76  Valid all:  84.01  Valid root:  91.62  Valid cost:  0.35243190181330236    time:  2971.436518907547\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "111  Train:  84.75  Train root:  92.89  Valid all:  84.02  Valid root:  91.56  Valid cost:  0.35307018889399117    time:  2971.7775995731354\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "112  Train:  85.10  Train root:  93.64  Valid all:  84.05  Valid root:  91.55  Valid cost:  0.3529523677130067    time:  2977.630598306656\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "113  Train:  84.60  Train root:  93.32  Valid all:  84.03  Valid root:  91.53  Valid cost:  0.3523016190202041    time:  2970.902346611023\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "114  Train:  84.78  Train root:  93.84  Valid all:  84.03  Valid root:  91.67  Valid cost:  0.3527377569208054    time:  2973.664964914322\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "115  Train:  84.89  Train root:  93.80  Valid all:  84.06  Valid root:  91.47  Valid cost:  0.35610268612593243    time:  2973.000142097473\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "116  Train:  84.91  Train root:  93.81  Valid all:  84.09  Valid root:  91.44  Valid cost:  0.3526305716371642    time:  2978.202700853348\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4507f34e835a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# and e%3==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-a4a7250db99a>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSharedVariable\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(68,200):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], 0.01)\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "117  Train:  84.87  Train root:  93.26  Valid all:  84.10  Valid root:  91.63  Valid cost:  0.35237926106869805    time:  3050.703408718109\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "118  Train:  84.90  Train root:  93.67  Valid all:  84.08  Valid root:  91.51  Valid cost:  0.35308801977468807    time:  2951.8431487083435\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "119  Train:  84.92  Train root:  93.54  Valid all:  84.05  Valid root:  91.43  Valid cost:  0.3532184570713174    time:  3019.144743204117\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "120  Train:  84.88  Train root:  93.77  Valid all:  84.02  Valid root:  91.48  Valid cost:  0.35254910421544855    time:  3226.6867797374725\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "121  Train:  84.77  Train root:  93.48  Valid all:  84.14  Valid root:  91.54  Valid cost:  0.3526093208609063    time:  4479.145063400269\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "122  Train:  85.02  Train root:  93.78  Valid all:  84.07  Valid root:  91.44  Valid cost:  0.3523204022506895    time:  3020.7851877212524\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "123  Train:  84.95  Train root:  93.58  Valid all:  84.10  Valid root:  91.45  Valid cost:  0.35107982905269675    time:  2851.7922763824463\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "124  Train:  85.22  Train root:  93.61  Valid all:  84.10  Valid root:  91.49  Valid cost:  0.3521104121867349    time:  3064.8841280937195\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "125  Train:  84.99  Train root:  93.69  Valid all:  84.11  Valid root:  91.55  Valid cost:  0.35241177017133185    time:  3262.2717740535736\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "126  Train:  84.75  Train root:  93.42  Valid all:  84.01  Valid root:  91.68  Valid cost:  0.352904679455092    time:  3250.763950586319\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "127  Train:  84.99  Train root:  93.66  Valid all:  84.08  Valid root:  91.52  Valid cost:  0.3506045359898742    time:  3233.6187403202057\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "128  Train:  84.60  Train root:  93.51  Valid all:  83.99  Valid root:  91.45  Valid cost:  0.35284072742819617    time:  2996.5668160915375\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "129  Train:  84.89  Train root:  93.65  Valid all:  84.10  Valid root:  91.51  Valid cost:  0.3527177247474358    time:  3200.4966928958893\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "130  Train:  84.74  Train root:  93.46  Valid all:  84.07  Valid root:  91.50  Valid cost:  0.35255659055108984    time:  3136.7847895622253\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "131  Train:  84.82  Train root:  93.81  Valid all:  84.09  Valid root:  91.51  Valid cost:  0.3517155046304781    time:  3081.6599271297455\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "132  Train:  84.97  Train root:  93.60  Valid all:  84.16  Valid root:  91.28  Valid cost:  0.35271206900398433    time:  3086.7283384799957\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "133  Train:  84.95  Train root:  93.65  Valid all:  84.03  Valid root:  91.24  Valid cost:  0.35353852370984606    time:  3094.1472613811493\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "134  Train:  85.07  Train root:  93.86  Valid all:  84.13  Valid root:  91.61  Valid cost:  0.3522773393353056    time:  3100.9277896881104\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "135  Train:  84.82  Train root:  93.63  Valid all:  84.00  Valid root:  91.50  Valid cost:  0.3525711880243202    time:  3109.228692293167\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "136  Train:  84.92  Train root:  93.66  Valid all:  83.99  Valid root:  91.50  Valid cost:  0.35185631790021793    time:  3117.52828168869\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "137  Train:  84.81  Train root:  93.63  Valid all:  84.04  Valid root:  91.20  Valid cost:  0.3547293929165747    time:  3124.9397060871124\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "138  Train:  84.95  Train root:  93.32  Valid all:  84.09  Valid root:  91.35  Valid cost:  0.350973596176547    time:  3137.4627475738525\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "139  Train:  85.01  Train root:  93.88  Valid all:  84.10  Valid root:  91.50  Valid cost:  0.3538546449286493    time:  3145.7789611816406\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "140  Train:  85.19  Train root:  94.08  Valid all:  84.16  Valid root:  91.62  Valid cost:  0.35111873599025323    time:  3153.651810646057\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "141  Train:  84.89  Train root:  93.52  Valid all:  84.04  Valid root:  91.43  Valid cost:  0.35089613168365924    time:  3250.8789281845093\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "142  Train:  85.00  Train root:  93.42  Valid all:  84.08  Valid root:  91.53  Valid cost:  0.35240419376477344    time:  3123.5124797821045\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "143  Train:  84.99  Train root:  93.02  Valid all:  84.08  Valid root:  91.52  Valid cost:  0.3510087650354016    time:  3041.3593049049377\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "144  Train:  84.65  Train root:  93.34  Valid all:  83.99  Valid root:  91.73  Valid cost:  0.3509199080019284    time:  3090.7227852344513\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "145  Train:  84.81  Train root:  93.57  Valid all:  84.06  Valid root:  91.65  Valid cost:  0.35251918794842824    time:  3055.9693043231964\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "146  Train:  84.82  Train root:  93.37  Valid all:  84.06  Valid root:  91.61  Valid cost:  0.35133549485035265    time:  3037.183571577072\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "147  Train:  84.87  Train root:  93.52  Valid all:  84.09  Valid root:  91.62  Valid cost:  0.3519322269553786    time:  3007.7625439167023\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "148  Train:  84.85  Train root:  93.61  Valid all:  84.07  Valid root:  91.54  Valid cost:  0.3515791897288168    time:  3025.48632645607\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "149  Train:  84.97  Train root:  93.71  Valid all:  84.11  Valid root:  91.60  Valid cost:  0.3527822227514535    time:  3207.6280121803284\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "150  Train:  84.93  Train root:  93.61  Valid all:  84.08  Valid root:  91.50  Valid cost:  0.35207144463432155    time:  3006.5524611473083\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "151  Train:  84.98  Train root:  93.37  Valid all:  84.12  Valid root:  91.64  Valid cost:  0.35151191320680275    time:  3063.612007856369\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "152  Train:  84.83  Train root:  93.14  Valid all:  84.07  Valid root:  91.62  Valid cost:  0.35130157802469597    time:  3388.9949584007263\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "153  Train:  85.01  Train root:  93.74  Valid all:  84.15  Valid root:  91.66  Valid cost:  0.3512680373337384    time:  3237.029819726944\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "154  Train:  84.76  Train root:  93.77  Valid all:  84.00  Valid root:  91.66  Valid cost:  0.35125424405182754    time:  3175.931390762329\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "155  Train:  85.01  Train root:  93.34  Valid all:  84.16  Valid root:  91.56  Valid cost:  0.35206739110242097    time:  3284.534205675125\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "156  Train:  84.90  Train root:  93.97  Valid all:  84.06  Valid root:  91.62  Valid cost:  0.3520889228140414    time:  3223.3965463638306\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f6d5ba40fcbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/norbert/.theano/compiledir_Linux-4.13--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.3-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(117,200):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], 0.01)\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "156  Train:  84.90  Train root:  93.24  Valid all:  84.14  Valid root:  91.55  Valid cost:  0.3502619148339196    time:  3023.6190757751465\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "157  Train:  84.80  Train root:  93.53  Valid all:  84.12  Valid root:  91.57  Valid cost:  0.34986446199176824    time:  3009.517916202545\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "158  Train:  85.07  Train root:  93.64  Valid all:  84.19  Valid root:  91.68  Valid cost:  0.3501333900644138    time:  3022.736661672592\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "159  Train:  84.97  Train root:  93.43  Valid all:  84.18  Valid root:  91.63  Valid cost:  0.34936022519692156    time:  3032.2502183914185\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "160  Train:  85.17  Train root:  93.95  Valid all:  84.19  Valid root:  91.68  Valid cost:  0.34942313577645256    time:  3030.857822418213\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "161  Train:  85.19  Train root:  93.81  Valid all:  84.17  Valid root:  91.67  Valid cost:  0.35115900239124076    time:  3040.1195225715637\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "162  Train:  85.05  Train root:  93.61  Valid all:  84.21  Valid root:  91.66  Valid cost:  0.34883900728756    time:  3047.820207118988\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "163  Train:  84.90  Train root:  93.79  Valid all:  84.17  Valid root:  91.63  Valid cost:  0.350503873902787    time:  3056.774428844452\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "164  Train:  84.77  Train root:  93.56  Valid all:  84.15  Valid root:  91.67  Valid cost:  0.35031397194051855    time:  3061.136675596237\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "165  Train:  85.10  Train root:  94.02  Valid all:  84.19  Valid root:  91.66  Valid cost:  0.3495784703739743    time:  3062.0317871570587\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "166  Train:  85.07  Train root:  93.39  Valid all:  84.19  Valid root:  91.66  Valid cost:  0.3505326140755384    time:  3091.4794528484344\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "167  Train:  84.99  Train root:  93.91  Valid all:  84.20  Valid root:  91.73  Valid cost:  0.35093383512121595    time:  3112.3051342964172\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "168  Train:  85.07  Train root:  93.70  Valid all:  84.19  Valid root:  91.69  Valid cost:  0.3492357584923149    time:  3019.0757319927216\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "169  Train:  84.96  Train root:  93.53  Valid all:  84.17  Valid root:  91.72  Valid cost:  0.3507228484687854    time:  3012.300368309021\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "170  Train:  84.92  Train root:  93.45  Valid all:  84.14  Valid root:  91.71  Valid cost:  0.34976845459094985    time:  3184.9414875507355\n",
      "0\n",
      "10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7977cf462171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/norbert/.theano/compiledir_Linux-4.13--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.3-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(156,200):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], 0.001)\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(171,200):\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(n_train):\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        rnn.train(train_data[i][0], train_data[i][4], train_data[i][1], train_data[i][2], train_data[i][3], 0.001)\n",
    "\n",
    "    loss = 0\n",
    "    counts_test = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_test_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in range(n_val):\n",
    "        pred = rnn.classify(validation_data[i][0],validation_data[i][4],validation_data[i][1], validation_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts_test[pred[j], validation_data[i][2][j]] += 1\n",
    "        counts_test_root[pred[-1], validation_data[i][2][-1]] += 1\n",
    "        loss += rnn.calculate_loss(validation_data[i][0], validation_data[i][4], validation_data[i][1], validation_data[i][2], validation_data[i][3], s['lr'])\n",
    "    \n",
    "    \n",
    "    # Train\n",
    "    counts = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    counts_root = np.zeros((s['nc'],s['nc']),dtype='int')\n",
    "    for i in random_train_trees:\n",
    "\n",
    "        pred  = rnn.classify(train_data[i][0], train_data[i][4] ,train_data[i][1], train_data[i][3])\n",
    "        for j in range(len(pred)):\n",
    "            counts[pred[j], train_data[i][2][j]] += 1\n",
    "        counts_root[pred[-1], train_data[i][2][-1]] += 1\n",
    "\n",
    "\n",
    "    if e>0:# and e%3==0:\n",
    "        \n",
    "        rnn.save_model(\"/home/norbert/Doktorat/SyntacticTreesDisambiguation/Model/model_params_\"+str(e)+\".pkl\")\n",
    "        \n",
    "\n",
    "    print(e, \" Train: \", \"%0.2f\" % (100 * np.diag(counts).sum()/float(counts.sum())),\n",
    "        \" Train root: \",\"%0.2f\" % (100 * np.diag(counts_root).sum()/float(counts_root.sum())),\n",
    "        \" Valid all: \",\"%0.2f\" % (100 * np.diag(counts_test).sum()/float(counts_test.sum())),\n",
    "        \" Valid root: \",\"%0.2f\" % (100 * np.diag(counts_test_root).sum()/float(counts_test_root.sum())), \n",
    "        \" Valid cost: \", loss/n_val,\n",
    "        \"   time: \", time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(pickle.load(open(\"model_params.pkl\",\"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0_test = load_stanford_data4(\"Data/test2/labels.txt\", \"Data/test2/parents.txt\",\"Data/test2/tokens.txt\",\"Data/test2/rules.txt\",w2vecs[\"words2ids\"],True,s['batch_size'],s['nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rules_test = load_stanford_data4(\"Data/test2/labels.txt\", \"Data/test2/parents.txt\",\"Data/test2/rules.txt\",\"Data/test2/rules.txt\",rnn.rules2ids,True,s['batch_size'],s['nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rules_test = [x[0] for x in data_rules_test]\n",
    "\n",
    "data_test = [data0_test[i]+[data_rules_test[i]] for i in range(len(data0_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for i in range(len(data_test)):\n",
    "    pred = rnn.predict_proba(data_test[i][0],data_test[i][4],data_test[i][1], data_test[i][3])[-1][1]\n",
    "    probs.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13334687,\n",
       " 8.3888835e-06,\n",
       " 0.0010396587,\n",
       " 4.6200694e-06,\n",
       " 5.0813065e-05,\n",
       " 1.4710424e-06,\n",
       " 2.0261099e-05,\n",
       " 3.289847e-06,\n",
       " 1.4634319e-06,\n",
       " 0.00012172688,\n",
       " 1.8303084e-06,\n",
       " 2.3695316e-05,\n",
       " 0.01863916,\n",
       " 1.8796135e-06,\n",
       " 4.4952594e-05,\n",
       " 1.7312494e-06,\n",
       " 2.489804e-05,\n",
       " 1.3020557e-05,\n",
       " 3.1925803e-05,\n",
       " 5.429791e-05,\n",
       " 4.196077e-05,\n",
       " 4.1475494e-05,\n",
       " 5.76528e-06,\n",
       " 8.790873e-07,\n",
       " 2.6987232e-06,\n",
       " 0.0002899247,\n",
       " 0.05050147,\n",
       " 1.1784717e-06,\n",
       " 0.0003152298,\n",
       " 7.109902e-05,\n",
       " 0.0001983568,\n",
       " 0.00057133636,\n",
       " 7.724816e-06,\n",
       " 0.00014834244,\n",
       " 9.692103e-06,\n",
       " 0.0033503277,\n",
       " 3.992597e-06,\n",
       " 5.3168696e-05,\n",
       " 0.000117437805,\n",
       " 1.6004777e-05,\n",
       " 6.539057e-05,\n",
       " 2.1992926e-05,\n",
       " 1.0912242e-05,\n",
       " 7.6280994e-05,\n",
       " 5.2460364e-06,\n",
       " 7.3081595e-05,\n",
       " 8.144737e-05,\n",
       " 1.4397174e-05,\n",
       " 6.1822852e-06,\n",
       " 1.658785e-05,\n",
       " 1.2030981e-05,\n",
       " 9.312199e-06,\n",
       " 0.00019822389,\n",
       " 0.0052587115,\n",
       " 0.00081432826,\n",
       " 3.5629741e-06,\n",
       " 9.307351e-06,\n",
       " 0.00015510345,\n",
       " 1.0968949e-05,\n",
       " 7.030539e-06,\n",
       " 5.0074805e-06,\n",
       " 6.1453115e-06,\n",
       " 2.8228587e-06,\n",
       " 3.7486498e-06,\n",
       " 1.4301316e-05,\n",
       " 1.3266325e-06,\n",
       " 1.0730317e-05,\n",
       " 1.6594464e-05,\n",
       " 1.6735402e-06,\n",
       " 8.700291e-06,\n",
       " 2.6172813e-05,\n",
       " 0.00013051779,\n",
       " 8.084965e-06,\n",
       " 5.483554e-05,\n",
       " 1.2153797e-05,\n",
       " 1.3478629e-05,\n",
       " 1.932376e-06,\n",
       " 7.001671e-05,\n",
       " 9.302408e-06,\n",
       " 0.0062965816,\n",
       " 9.907748e-07,\n",
       " 1.6036396e-06,\n",
       " 6.2852614e-06,\n",
       " 0.000251852,\n",
       " 2.4793171e-05,\n",
       " 1.6088623e-05,\n",
       " 4.0713954e-07,\n",
       " 1.5990033e-06,\n",
       " 4.385126e-05,\n",
       " 6.9087764e-05,\n",
       " 3.204094e-05,\n",
       " 3.0822373e-06,\n",
       " 6.434213e-06,\n",
       " 5.4155185e-05,\n",
       " 7.715648e-05,\n",
       " 1.4065533e-05,\n",
       " 8.094841e-05,\n",
       " 8.452274e-06,\n",
       " 7.803256e-07,\n",
       " 0.00017794677,\n",
       " 4.8403913e-06,\n",
       " 0.00086278614,\n",
       " 6.712194e-05,\n",
       " 0.025859497,\n",
       " 3.067306e-06,\n",
       " 1.0328978e-05,\n",
       " 9.930138e-07,\n",
       " 1.5877768e-06,\n",
       " 1.2383073e-05,\n",
       " 5.152706e-06,\n",
       " 1.3909213e-05,\n",
       " 0.00056928245,\n",
       " 1.4472916e-05,\n",
       " 1.8795397e-05,\n",
       " 4.0774985e-06,\n",
       " 3.9316405e-05,\n",
       " 7.961795e-05,\n",
       " 4.9726646e-06,\n",
       " 4.6472774e-06,\n",
       " 0.00022784575,\n",
       " 2.5461875e-06,\n",
       " 7.9791316e-05,\n",
       " 2.64324e-05,\n",
       " 3.0270642e-06,\n",
       " 5.5246524e-05,\n",
       " 1.4181959e-06,\n",
       " 8.800043e-06,\n",
       " 4.6235204e-05,\n",
       " 2.6652819e-05,\n",
       " 8.18751e-06,\n",
       " 6.140613e-06,\n",
       " 1.2444029e-05,\n",
       " 7.742724e-06,\n",
       " 4.656771e-06,\n",
       " 1.5624986e-05,\n",
       " 5.479629e-06,\n",
       " 9.704977e-06,\n",
       " 9.346909e-05,\n",
       " 8.183965e-06,\n",
       " 1.0706991e-05,\n",
       " 1.0857568e-05,\n",
       " 9.228014e-05,\n",
       " 9.073281e-06,\n",
       " 6.8863174e-06,\n",
       " 3.832817e-06,\n",
       " 0.00013432815,\n",
       " 9.424707e-06,\n",
       " 1.7967954e-05,\n",
       " 3.615448e-05,\n",
       " 5.6692927e-05,\n",
       " 0.00017307086,\n",
       " 2.3469115e-06,\n",
       " 1.4772135e-05,\n",
       " 1.3371289e-05,\n",
       " 0.000141575,\n",
       " 3.5007477e-06,\n",
       " 1.7365808e-05,\n",
       " 1.19442e-05,\n",
       " 8.308707e-05,\n",
       " 1.2075616e-05,\n",
       " 2.315082e-05,\n",
       " 1.1145954e-06,\n",
       " 4.7141988e-05,\n",
       " 4.2044765e-05,\n",
       " 3.5627736e-06,\n",
       " 2.2375503e-05,\n",
       " 1.9250027e-05,\n",
       " 2.0105356e-06,\n",
       " 1.4457631e-05,\n",
       " 6.2511044e-06,\n",
       " 2.3186822e-06,\n",
       " 0.00018604082,\n",
       " 3.7417885e-06,\n",
       " 2.5372567e-05,\n",
       " 1.955376e-06,\n",
       " 7.771931e-06,\n",
       " 0.019495666,\n",
       " 1.1378954e-05,\n",
       " 3.293336e-05,\n",
       " 2.1107396e-06,\n",
       " 1.4635417e-05,\n",
       " 5.125115e-05,\n",
       " 9.7943675e-06,\n",
       " 3.785751e-06,\n",
       " 1.31206e-05,\n",
       " 2.2820206e-05,\n",
       " 2.0814767e-05,\n",
       " 3.3805503e-05,\n",
       " 2.6959706e-06,\n",
       " 0.0005181706,\n",
       " 5.2847167e-06,\n",
       " 9.662417e-05,\n",
       " 3.560783e-06,\n",
       " 8.953381e-06,\n",
       " 9.034935e-06,\n",
       " 7.288778e-06,\n",
       " 0.00042016065,\n",
       " 2.8668841e-05,\n",
       " 9.7999175e-05,\n",
       " 0.002829116,\n",
       " 8.211196e-06,\n",
       " 9.822167e-06,\n",
       " 8.787312e-06,\n",
       " 2.3354196e-06,\n",
       " 6.3001567e-06,\n",
       " 5.3690797e-05,\n",
       " 1.5218998e-05,\n",
       " 3.876917e-05,\n",
       " 6.853912e-06,\n",
       " 5.0535837e-06,\n",
       " 2.0550546e-05,\n",
       " 1.5924043e-05,\n",
       " 1.532789e-05,\n",
       " 1.4400456e-05,\n",
       " 3.825165e-05,\n",
       " 7.0193914e-06,\n",
       " 0.00014046405,\n",
       " 7.718349e-05,\n",
       " 4.48689e-06,\n",
       " 1.8762194e-05,\n",
       " 9.627431e-05,\n",
       " 6.0569305e-06,\n",
       " 2.107386e-05,\n",
       " 5.913103e-06,\n",
       " 3.5850912e-06,\n",
       " 0.0016458663,\n",
       " 2.5142253e-06,\n",
       " 1.0782994e-05,\n",
       " 5.166837e-05,\n",
       " 1.9547311e-05,\n",
       " 1.1447011e-05,\n",
       " 6.272734e-06,\n",
       " 0.0001826703,\n",
       " 8.144079e-06,\n",
       " 7.4821687e-06,\n",
       " 3.0552515e-06,\n",
       " 1.2106057e-05,\n",
       " 4.0265622e-05,\n",
       " 1.8122171e-06,\n",
       " 8.475073e-05,\n",
       " 0.00020921935,\n",
       " 1.3235717e-05,\n",
       " 1.43279085e-05,\n",
       " 2.6418852e-06,\n",
       " 5.0170078e-05,\n",
       " 0.00013212372,\n",
       " 0.0004856708,\n",
       " 0.00011975081,\n",
       " 3.410176e-06,\n",
       " 2.9618293e-05,\n",
       " 3.9040237e-06,\n",
       " 2.393912e-05,\n",
       " 7.6968725e-05,\n",
       " 1.0965351e-05,\n",
       " 2.5917952e-05,\n",
       " 7.0230555e-05,\n",
       " 0.00043710225,\n",
       " 3.8155486e-06,\n",
       " 0.000104274855,\n",
       " 1.3998998e-05,\n",
       " 1.6201884e-05,\n",
       " 1.5043358e-05,\n",
       " 1.914724e-05,\n",
       " 5.6697468e-06,\n",
       " 4.791963e-06,\n",
       " 2.2797778e-05,\n",
       " 4.4781103e-05,\n",
       " 1.0429324e-05,\n",
       " 7.657062e-05,\n",
       " 6.5218633e-06,\n",
       " 1.6637294e-05,\n",
       " 4.6944724e-06,\n",
       " 1.692348e-05,\n",
       " 2.5159068e-06,\n",
       " 1.825818e-05,\n",
       " 0.00011265903,\n",
       " 9.1778446e-07,\n",
       " 1.7059245e-06,\n",
       " 4.686721e-05,\n",
       " 2.1771331e-05,\n",
       " 3.3448682e-06,\n",
       " 3.6438723e-06,\n",
       " 2.897359e-06,\n",
       " 1.349625e-05,\n",
       " 9.326142e-06,\n",
       " 2.990472e-05,\n",
       " 1.4576163e-05,\n",
       " 4.2881143e-06,\n",
       " 0.00011429456,\n",
       " 8.869891e-06,\n",
       " 1.0214543e-05,\n",
       " 1.2246082e-05,\n",
       " 8.489082e-05,\n",
       " 6.7775513e-06,\n",
       " 0.00015151016,\n",
       " 0.0009065243,\n",
       " 8.750866e-07,\n",
       " 5.8979414e-05,\n",
       " 7.5480074e-07,\n",
       " 1.7089358e-05,\n",
       " 2.5002519e-05,\n",
       " 2.7020271e-06,\n",
       " 8.0956823e-07,\n",
       " 1.5537495e-05,\n",
       " 5.517355e-05,\n",
       " 1.0205723e-06,\n",
       " 7.140607e-05,\n",
       " 4.9769774e-06,\n",
       " 0.00025494225,\n",
       " 2.0126497e-06,\n",
       " 4.1262083e-06,\n",
       " 8.259886e-05,\n",
       " 9.577233e-06,\n",
       " 2.1718035e-06,\n",
       " 1.4211028e-05,\n",
       " 5.262402e-06,\n",
       " 1.9105617e-05,\n",
       " 9.729152e-05,\n",
       " 2.1270864e-05,\n",
       " 2.6139114e-05,\n",
       " 2.125451e-06,\n",
       " 2.203857e-05,\n",
       " 7.445051e-05,\n",
       " 0.00010750937,\n",
       " 3.6008623e-05,\n",
       " 1.644444e-05,\n",
       " 1.70516e-06,\n",
       " 0.0006819584,\n",
       " 8.0066624e-05,\n",
       " 4.8302325e-05,\n",
       " 0.00012514656,\n",
       " 8.980334e-05,\n",
       " 1.7438115e-05,\n",
       " 2.30134e-05,\n",
       " 3.8563672e-07,\n",
       " 8.980334e-05,\n",
       " 4.186648e-06,\n",
       " 5.6416784e-06,\n",
       " 7.4404143e-06,\n",
       " 6.809518e-06,\n",
       " 1.1592689e-06,\n",
       " 5.133655e-06,\n",
       " 9.87498e-06,\n",
       " 5.682262e-06,\n",
       " 1.1222409e-06,\n",
       " 6.7061286e-05,\n",
       " 0.000101118945,\n",
       " 1.93201e-05,\n",
       " 3.6352685e-05,\n",
       " 5.8089663e-05,\n",
       " 4.087949e-06,\n",
       " 2.4650184e-05,\n",
       " 7.4704467e-06,\n",
       " 0.000106999636,\n",
       " 8.163979e-06,\n",
       " 1.2710573e-05,\n",
       " 2.3053817e-05,\n",
       " 2.3905037e-05,\n",
       " 3.4968068e-06,\n",
       " 5.306949e-06,\n",
       " 0.0014869247,\n",
       " 1.073906e-05,\n",
       " 9.872371e-06,\n",
       " 1.0964933e-05,\n",
       " 2.813444e-06,\n",
       " 4.813352e-06,\n",
       " 4.2762063e-06,\n",
       " 1.0298623e-05,\n",
       " 1.7016146e-05,\n",
       " 4.366944e-05,\n",
       " 4.5821885e-06,\n",
       " 1.6650341e-05,\n",
       " 1.6054812e-05,\n",
       " 4.0324626e-06,\n",
       " 0.00011197082,\n",
       " 1.8755738e-06,\n",
       " 4.3696627e-06,\n",
       " 4.856694e-05,\n",
       " 8.699017e-05,\n",
       " 2.5033843e-05,\n",
       " 6.660178e-05,\n",
       " 1.00209445e-05,\n",
       " 2.984754e-05,\n",
       " 4.998177e-06,\n",
       " 0.0026496374,\n",
       " 3.9157285e-06,\n",
       " 4.9968778e-05,\n",
       " 1.4953219e-05,\n",
       " 7.076191e-05,\n",
       " 4.52246e-06,\n",
       " 3.5537878e-06,\n",
       " 9.206717e-05,\n",
       " 2.2202437e-05,\n",
       " 2.7792012e-05,\n",
       " 4.2879634e-05,\n",
       " 1.2444029e-05,\n",
       " 1.8647655e-05,\n",
       " 5.0834074e-06,\n",
       " 1.7597187e-05,\n",
       " 0.0007897245,\n",
       " 3.7027057e-06,\n",
       " 1.2254704e-05,\n",
       " 2.4375951e-05,\n",
       " 2.8233e-05,\n",
       " 1.8417986e-05,\n",
       " 2.7104872e-05,\n",
       " 1.2514245e-05,\n",
       " 3.951166e-06,\n",
       " 5.3248955e-06,\n",
       " 5.872405e-06,\n",
       " 2.1154128e-06,\n",
       " 3.4936702e-06,\n",
       " 2.4917328e-05,\n",
       " 2.7392225e-06,\n",
       " 0.0001609692,\n",
       " 5.5195338e-05,\n",
       " 5.4286243e-06,\n",
       " 8.062413e-06,\n",
       " 8.21172e-06,\n",
       " 2.6618645e-06,\n",
       " 5.737083e-06,\n",
       " 9.122554e-06,\n",
       " 8.7805586e-07,\n",
       " 9.780014e-05,\n",
       " 1.1540829e-05,\n",
       " 5.0349668e-06,\n",
       " 8.960275e-06,\n",
       " 2.8234615e-05,\n",
       " 0.0005757781,\n",
       " 2.8806171e-05,\n",
       " 2.2362715e-06,\n",
       " 2.1584592e-05,\n",
       " 2.0734824e-06,\n",
       " 1.9951896e-05,\n",
       " 4.5948e-06,\n",
       " 9.780014e-05,\n",
       " 3.9627243e-06,\n",
       " 2.199666e-05,\n",
       " 4.3125897e-06,\n",
       " 7.3870906e-06,\n",
       " 3.807122e-05,\n",
       " 0.00037188726,\n",
       " 1.34824595e-05,\n",
       " 9.71433e-06,\n",
       " 2.0311632e-05,\n",
       " 4.8452407e-06,\n",
       " 2.242006e-05,\n",
       " 4.544046e-06,\n",
       " 0.00016705865,\n",
       " 8.6759866e-07,\n",
       " 7.0949136e-05,\n",
       " 0.00015158141,\n",
       " 2.2461556e-05,\n",
       " 3.0029694e-05,\n",
       " 5.479943e-06,\n",
       " 5.714057e-06,\n",
       " 6.7145616e-06,\n",
       " 1.1957384e-06,\n",
       " 3.397777e-05,\n",
       " 7.704804e-06,\n",
       " 9.447487e-05,\n",
       " 0.00016936859,\n",
       " 3.1149611e-06,\n",
       " 1.7840748e-05,\n",
       " 5.814592e-05,\n",
       " 2.223386e-05,\n",
       " 4.3598766e-06,\n",
       " 1.1456513e-05,\n",
       " 2.9709468e-05,\n",
       " 1.5108483e-06,\n",
       " 2.9116713e-06,\n",
       " 0.00011181588,\n",
       " 9.236527e-05,\n",
       " 1.8820242e-06,\n",
       " 7.780319e-05,\n",
       " 1.0485302e-05,\n",
       " 9.9295416e-05,\n",
       " 2.7354627e-05,\n",
       " 0.007163331,\n",
       " 9.551957e-06,\n",
       " 0.00045539252,\n",
       " 3.9292813e-06,\n",
       " 4.02845e-06,\n",
       " 0.00029718934,\n",
       " 5.349153e-06,\n",
       " 6.716074e-05,\n",
       " 4.375648e-05,\n",
       " 6.5934314e-06,\n",
       " 5.1660104e-06,\n",
       " 8.069051e-06,\n",
       " 9.3607537e-07,\n",
       " 5.03967e-05,\n",
       " 0.0001299655,\n",
       " 8.493655e-06,\n",
       " 1.9533298e-05,\n",
       " 0.0002817492,\n",
       " 2.990472e-05,\n",
       " 8.789211e-05,\n",
       " 2.3186003e-06,\n",
       " 2.1870239e-05,\n",
       " 0.0001826797,\n",
       " 6.3137313e-06,\n",
       " 1.8597024e-05,\n",
       " 1.7408936e-05,\n",
       " 5.423524e-05,\n",
       " 5.224279e-06,\n",
       " 3.4964867e-06,\n",
       " 0.00015679417,\n",
       " 2.3770639e-05,\n",
       " 2.9096784e-06,\n",
       " 1.9246687e-05,\n",
       " 5.9796043e-06,\n",
       " 2.000593e-05,\n",
       " 2.1389147e-05,\n",
       " 1.8476021e-06,\n",
       " 0.00736165,\n",
       " 1.1138539e-05,\n",
       " 3.4098062e-05,\n",
       " 1.2211341e-05,\n",
       " 9.4473966e-05,\n",
       " 0.001027711,\n",
       " 7.792949e-05,\n",
       " 2.5923886e-05,\n",
       " 6.4194232e-06,\n",
       " 0.00010887542,\n",
       " 3.17661e-06,\n",
       " 1.9880386e-05,\n",
       " 0.00012601269,\n",
       " 1.5373857e-06,\n",
       " 2.2772812e-05,\n",
       " 5.8760917e-05,\n",
       " 4.549243e-05,\n",
       " 2.3907727e-05,\n",
       " 4.5141824e-06,\n",
       " 5.834235e-06,\n",
       " 4.609081e-06,\n",
       " 1.5095723e-06,\n",
       " 0.0027053477,\n",
       " 0.021328993,\n",
       " 1.0111759e-06,\n",
       " 7.4628566e-05,\n",
       " 0.00040242155,\n",
       " 5.4574857e-06,\n",
       " 4.553106e-05,\n",
       " 2.6829232e-06,\n",
       " 7.8897145e-05,\n",
       " 5.565878e-05,\n",
       " 0.00013912334,\n",
       " 0.0001380765,\n",
       " 8.80236e-06,\n",
       " 3.0661115e-05,\n",
       " 4.2398526e-05,\n",
       " 2.1361466e-05,\n",
       " 6.543325e-05,\n",
       " 8.888198e-06,\n",
       " 6.9291896e-06,\n",
       " 3.0174317e-06,\n",
       " 8.9439945e-06,\n",
       " 0.00024813213,\n",
       " 6.595893e-05,\n",
       " 3.0275896e-06,\n",
       " 3.754074e-05,\n",
       " 7.241659e-07,\n",
       " 2.6910745e-06,\n",
       " 9.815781e-06,\n",
       " 2.1627655e-05,\n",
       " 2.297544e-05,\n",
       " 9.790605e-05,\n",
       " 2.213435e-06,\n",
       " 5.1302586e-06,\n",
       " 1.3781349e-05,\n",
       " 8.489832e-06,\n",
       " 0.0004626695,\n",
       " 0.00017450689,\n",
       " 0.00035161254,\n",
       " 1.8044514e-06,\n",
       " 2.2272037e-05,\n",
       " 1.0605145e-05,\n",
       " 9.2552983e-07,\n",
       " 3.3700865e-06,\n",
       " 0.00015853415,\n",
       " 2.0202351e-05,\n",
       " 9.490341e-05,\n",
       " 3.1940895e-06,\n",
       " 2.6005993e-05,\n",
       " 1.2178266e-06,\n",
       " 5.736919e-06,\n",
       " 4.4790618e-06,\n",
       " 1.5490004e-06,\n",
       " 4.601403e-07,\n",
       " 3.316339e-06,\n",
       " 2.7185201e-06,\n",
       " 5.696764e-05,\n",
       " 1.9749605e-06,\n",
       " 0.000101116246,\n",
       " 1.6568279e-06,\n",
       " 4.09895e-06,\n",
       " 5.3011972e-06,\n",
       " 3.4078756e-05,\n",
       " 2.5811569e-05,\n",
       " 5.566998e-05,\n",
       " 2.1845935e-06,\n",
       " 2.1261626e-06,\n",
       " 1.3475389e-05,\n",
       " 1.7501925e-05,\n",
       " 9.4031634e-07,\n",
       " 7.5071844e-06,\n",
       " 0.0031265572,\n",
       " 2.7936137e-06,\n",
       " 1.9207807e-06,\n",
       " 2.876447e-05,\n",
       " 6.084663e-06,\n",
       " 3.9376014e-06,\n",
       " 0.00018654583,\n",
       " 3.4031066e-06,\n",
       " 1.7102207e-05,\n",
       " 4.2190848e-05,\n",
       " 0.00069706846,\n",
       " 1.2960398e-05,\n",
       " 1.2545722e-06,\n",
       " 6.1412984e-06,\n",
       " 1.2681738e-06,\n",
       " 2.8250158e-06,\n",
       " 2.6618798e-06,\n",
       " 6.5323584e-06,\n",
       " 8.264026e-06,\n",
       " 9.250708e-06,\n",
       " 0.00040266715,\n",
       " 3.0538933e-05,\n",
       " 4.510013e-06,\n",
       " 2.27888e-06,\n",
       " 3.42307e-05,\n",
       " 0.00042016065,\n",
       " 1.0025598e-05,\n",
       " 0.00018604082,\n",
       " 4.446491e-06,\n",
       " 5.5866647e-05,\n",
       " 2.085389e-06,\n",
       " 7.1248134e-05,\n",
       " 5.8743653e-06,\n",
       " 6.191679e-06,\n",
       " 9.00148e-06,\n",
       " 1.2517551e-05,\n",
       " 2.701414e-05,\n",
       " 6.656511e-06,\n",
       " 0.00013065651,\n",
       " 6.246653e-06,\n",
       " 1.4466584e-06,\n",
       " 1.9327576e-06,\n",
       " 2.7225906e-05,\n",
       " 6.928642e-05,\n",
       " 3.2736963e-05,\n",
       " 8.220622e-06,\n",
       " 5.272484e-06,\n",
       " 6.023216e-06,\n",
       " 2.6398897e-05,\n",
       " 7.3705286e-07,\n",
       " 7.6540106e-05,\n",
       " 1.4585803e-06,\n",
       " 4.348275e-06,\n",
       " 3.090085e-05,\n",
       " 5.518881e-05,\n",
       " 2.2430153e-05,\n",
       " 1.4157584e-05,\n",
       " 1.0408293e-06,\n",
       " 3.4644196e-05,\n",
       " 2.2514505e-05,\n",
       " 8.512185e-05,\n",
       " 5.226109e-05,\n",
       " 8.707679e-06,\n",
       " 1.04390665e-05,\n",
       " 1.9877264e-06,\n",
       " 5.280354e-06,\n",
       " 8.353456e-07,\n",
       " 8.981241e-05,\n",
       " 0.00028205774,\n",
       " 0.0003669399,\n",
       " 0.00016847129,\n",
       " 0.0025393846,\n",
       " 1.3520395e-06,\n",
       " 1.1799106e-05,\n",
       " 2.25702e-06,\n",
       " 6.416424e-06,\n",
       " 1.4827914e-05,\n",
       " 4.0602903e-05,\n",
       " 6.2307954e-05,\n",
       " 9.241669e-06,\n",
       " 2.7177477e-06,\n",
       " 8.382177e-05,\n",
       " 1.7236006e-06,\n",
       " 6.089766e-05,\n",
       " 2.742081e-05,\n",
       " 1.5803678e-05,\n",
       " 0.0001567281,\n",
       " 5.4180186e-05,\n",
       " 0.00014225201,\n",
       " 6.747189e-06,\n",
       " 3.0181252e-05,\n",
       " 2.4152891e-06,\n",
       " 0.00011391364,\n",
       " 9.174588e-06,\n",
       " 2.6808113e-05,\n",
       " 1.4913443e-05,\n",
       " 1.8320843e-06,\n",
       " 0.009730916,\n",
       " 4.388314e-05,\n",
       " 3.017196e-05,\n",
       " 1.5389158e-05,\n",
       " 4.309465e-06,\n",
       " 1.9652358e-05,\n",
       " 6.645119e-06,\n",
       " 0.0010857955,\n",
       " 0.0003509282,\n",
       " 8.2555394e-05,\n",
       " 5.2045092e-05,\n",
       " 7.646059e-05,\n",
       " 5.2563373e-05,\n",
       " 7.637054e-07,\n",
       " 1.4977367e-05,\n",
       " 1.6405453e-05,\n",
       " 4.1544713e-06,\n",
       " 6.0754896e-06,\n",
       " 1.98056e-05,\n",
       " 1.48280415e-05,\n",
       " 6.5106283e-06,\n",
       " 8.9783825e-06,\n",
       " 8.425933e-05,\n",
       " 2.3349185e-06,\n",
       " 1.423615e-05,\n",
       " 4.003166e-06,\n",
       " 9.3513645e-06,\n",
       " 3.3506726e-06,\n",
       " 9.2721435e-05,\n",
       " 3.0391835e-05,\n",
       " 1.2466135e-05,\n",
       " 5.423284e-06,\n",
       " 4.4516073e-06,\n",
       " 0.0027848976,\n",
       " 0.00028770094,\n",
       " 8.894651e-06,\n",
       " 1.0594339e-05,\n",
       " 0.017302357,\n",
       " 3.118002e-05,\n",
       " 0.00020156382,\n",
       " 3.9165858e-05,\n",
       " 1.0257368e-05,\n",
       " 4.61171e-06,\n",
       " 5.300275e-05,\n",
       " 1.3253224e-05,\n",
       " 0.00035122375,\n",
       " 2.2078371e-05,\n",
       " 2.413233e-06,\n",
       " 9.568744e-05,\n",
       " 1.4074724e-05,\n",
       " 1.7926926e-05,\n",
       " 0.000120884564,\n",
       " 7.3525393e-06,\n",
       " 1.9658355e-06,\n",
       " 1.4710424e-06,\n",
       " 2.6964517e-05,\n",
       " 1.7839115e-05,\n",
       " 9.457914e-06,\n",
       " 5.5454915e-05,\n",
       " 1.1926786e-05,\n",
       " 1.1410608e-05,\n",
       " 1.2014078e-06,\n",
       " 1.0676199e-05,\n",
       " 2.1725704e-05,\n",
       " 0.00010614923,\n",
       " 5.003442e-06,\n",
       " 0.00014848326,\n",
       " 5.0813065e-05,\n",
       " 4.0624777e-06,\n",
       " 7.8904144e-05,\n",
       " 0.00026887117,\n",
       " 1.4501016e-05,\n",
       " 6.0265298e-05,\n",
       " 3.1030515e-05,\n",
       " 1.9079662e-06,\n",
       " 5.368634e-05,\n",
       " 3.8483475e-05,\n",
       " 0.00012460821,\n",
       " 5.5232385e-06,\n",
       " 6.192494e-06,\n",
       " 1.8488464e-06,\n",
       " 2.4603236e-05,\n",
       " 2.1006841e-05,\n",
       " 3.0452238e-05,\n",
       " 1.6429642e-05,\n",
       " 5.340111e-06,\n",
       " 5.0663166e-05,\n",
       " 6.47655e-06,\n",
       " 0.00036218567,\n",
       " 9.157962e-06,\n",
       " 3.3707995e-05,\n",
       " 4.054695e-05,\n",
       " 2.9939249e-05,\n",
       " 3.0463223e-06,\n",
       " 5.572527e-05,\n",
       " 6.218663e-06,\n",
       " 9.75338e-07,\n",
       " 2.3285334e-05,\n",
       " 1.05989875e-05,\n",
       " 6.4387415e-05,\n",
       " 1.3743477e-06,\n",
       " 5.333404e-05,\n",
       " 0.00031107696,\n",
       " 0.00028007638,\n",
       " 6.83132e-06,\n",
       " 1.3027537e-05,\n",
       " 3.0419846e-05,\n",
       " 0.00027717452,\n",
       " 1.3599494e-05,\n",
       " 4.1387443e-06,\n",
       " 8.1388294e-05,\n",
       " 1.3282592e-05,\n",
       " 2.2312504e-06,\n",
       " 4.8825814e-06,\n",
       " 9.80274e-05,\n",
       " 1.45443655e-05,\n",
       " 7.1772756e-06,\n",
       " 2.7708904e-06,\n",
       " 3.5061385e-05,\n",
       " 0.0001166731,\n",
       " 1.3941558e-06,\n",
       " 2.9142661e-06,\n",
       " 2.1130692e-05,\n",
       " 1.8949675e-05,\n",
       " 0.00019465179,\n",
       " 4.4454137e-06,\n",
       " 3.21946e-06,\n",
       " 2.6203113e-06,\n",
       " 0.0003530853,\n",
       " 2.7110404e-05,\n",
       " 1.5365459e-06,\n",
       " 6.959607e-06,\n",
       " 6.735989e-05,\n",
       " 4.3831856e-06,\n",
       " 3.709536e-05,\n",
       " 5.0717696e-05,\n",
       " 8.722766e-05,\n",
       " 8.976722e-06,\n",
       " 2.9718422e-05,\n",
       " 5.9985152e-05,\n",
       " 9.34434e-06,\n",
       " 3.945315e-06,\n",
       " 1.0893828e-05,\n",
       " 1.7173039e-05,\n",
       " 0.00031629787,\n",
       " 7.851344e-06,\n",
       " 5.532691e-06,\n",
       " 2.3890907e-05,\n",
       " 0.023069428,\n",
       " 1.508315e-05,\n",
       " 9.934155e-06,\n",
       " 1.2497443e-06,\n",
       " 0.00010404076,\n",
       " 7.808236e-07,\n",
       " 0.00015186018,\n",
       " 2.9985797e-05,\n",
       " 6.4422304e-05,\n",
       " 0.00048927474,\n",
       " 2.899103e-06,\n",
       " 0.0001347858,\n",
       " 3.8066064e-05,\n",
       " 4.5989083e-05,\n",
       " 1.1077034e-05,\n",
       " 2.852845e-06,\n",
       " 0.000108047905,\n",
       " 3.361679e-05,\n",
       " 5.37875e-05,\n",
       " 8.450791e-06,\n",
       " 3.9260676e-06,\n",
       " 6.998403e-06,\n",
       " 0.00014539364,\n",
       " 0.0001758448,\n",
       " 9.902497e-06,\n",
       " 8.0879425e-07,\n",
       " 2.3622666e-05,\n",
       " 1.7645435e-06,\n",
       " 9.2171405e-07,\n",
       " 0.00036218567,\n",
       " 0.00020504325,\n",
       " 8.272668e-06,\n",
       " 2.7890408e-05,\n",
       " 0.00092524383,\n",
       " 1.8557765e-05,\n",
       " 1.6222984e-06,\n",
       " 1.36572135e-05,\n",
       " 5.0621446e-05,\n",
       " 3.4775749e-06,\n",
       " 1.8309473e-06,\n",
       " 8.1923856e-05,\n",
       " 4.9128204e-07,\n",
       " 3.0838485e-06,\n",
       " 4.2484107e-06,\n",
       " 0.0015397912,\n",
       " 1.490128e-06,\n",
       " 1.1370687e-05,\n",
       " 0.00014389251,\n",
       " 7.990011e-06,\n",
       " 2.7974663e-06,\n",
       " 2.0410818e-05,\n",
       " 2.8510065e-06,\n",
       " 6.7146702e-06,\n",
       " 5.628589e-05,\n",
       " 3.842025e-06,\n",
       " 6.013171e-05,\n",
       " 3.1934742e-06,\n",
       " 2.9957664e-06,\n",
       " 0.00014394561,\n",
       " 6.020224e-06,\n",
       " 2.8244795e-05,\n",
       " 3.0264115e-05,\n",
       " 0.000105416504,\n",
       " 1.306557e-06,\n",
       " 5.09857e-06,\n",
       " 5.871353e-05,\n",
       " 0.000104710394,\n",
       " 3.232776e-06,\n",
       " 0.00012080252,\n",
       " 2.7258744e-05,\n",
       " 1.8093039e-06,\n",
       " 0.00025897298,\n",
       " 3.1544184e-06,\n",
       " 4.781404e-06,\n",
       " 9.206717e-05,\n",
       " 6.817635e-05,\n",
       " 4.7444246e-05,\n",
       " 7.001971e-05,\n",
       " 1.4850104e-05,\n",
       " 6.6135335e-06,\n",
       " 6.824861e-06,\n",
       " 3.250598e-05,\n",
       " 1.7272901e-05,\n",
       " 8.714551e-05,\n",
       " 5.8659234e-06,\n",
       " 1.0008919e-05,\n",
       " 1.1812594e-05,\n",
       " 1.0548076e-05,\n",
       " 1.963089e-05,\n",
       " 0.020645004,\n",
       " 0.00036318717,\n",
       " 4.9729965e-06,\n",
       " 6.294865e-06,\n",
       " 3.2437763e-06,\n",
       " 1.2605816e-05,\n",
       " 1.8492115e-05,\n",
       " 5.8495166e-06,\n",
       " 0.003928794,\n",
       " 3.3796412e-05,\n",
       " 8.826434e-06,\n",
       " 2.4773715e-06,\n",
       " 8.537353e-06,\n",
       " 3.1224892e-05,\n",
       " 1.5535719e-06,\n",
       " 4.2710595e-05,\n",
       " 4.9998805e-05,\n",
       " 3.5659416e-06,\n",
       " 3.262511e-05,\n",
       " 5.768151e-05,\n",
       " 1.0912804e-05,\n",
       " 3.8974954e-06,\n",
       " 2.045072e-05,\n",
       " 9.323794e-06,\n",
       " 6.8878544e-06,\n",
       " 4.4965022e-06,\n",
       " 1.2529398e-05,\n",
       " 9.276168e-05,\n",
       " 2.480071e-06,\n",
       " 5.4343545e-05,\n",
       " 7.5586495e-06,\n",
       " 0.0001027102,\n",
       " 3.6485566e-05,\n",
       " 3.8281352e-05,\n",
       " 1.9538697e-06,\n",
       " 0.0015004635,\n",
       " 0.00013127117,\n",
       " 1.6294231e-06,\n",
       " 2.7989847e-06,\n",
       " 3.1965435e-05,\n",
       " 3.6608282e-05,\n",
       " 2.9036744e-05,\n",
       " 1.5435737e-05,\n",
       " 2.3780885e-06,\n",
       " 6.7869136e-05,\n",
       " 0.00018598036,\n",
       " 1.7876853e-05,\n",
       " 1.1221446e-05,\n",
       " 6.4779642e-06,\n",
       " 9.140625e-07,\n",
       " 0.00040546054,\n",
       " 6.53115e-06,\n",
       " 4.3611867e-06,\n",
       " 2.390727e-05,\n",
       " 3.5735702e-05,\n",
       " 2.57067e-05,\n",
       " 7.385765e-05,\n",
       " 0.00040841068,\n",
       " 1.2879645e-05,\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f1 = pickle.load(open(\"f1_2.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.75,\n",
       " 0.849056603773585,\n",
       " 0.7177033492822966,\n",
       " 0.75,\n",
       " 0.7081339712918661,\n",
       " 0.7707317073170732,\n",
       " 0.6570048309178744,\n",
       " 0.7203791469194313,\n",
       " 0.75,\n",
       " 0.6255924170616113,\n",
       " 0.8115942028985507,\n",
       " 0.8909952606635071,\n",
       " 0.7867298578199053,\n",
       " 0.6540284360189573,\n",
       " 0.7169811320754716,\n",
       " 0.6826923076923077,\n",
       " 0.7677725118483413,\n",
       " 0.6729857819905213,\n",
       " 0.8058252427184467,\n",
       " 0.7246376811594204,\n",
       " 0.7692307692307694,\n",
       " 0.6923076923076923,\n",
       " 0.6411483253588517,\n",
       " 0.6919431279620852,\n",
       " 0.7902439024390245,\n",
       " 0.8056872037914692,\n",
       " 0.6919431279620852,\n",
       " 0.8502415458937197,\n",
       " 0.7609756097560976,\n",
       " 0.7019230769230769,\n",
       " 0.7536231884057971,\n",
       " 0.6504854368932039,\n",
       " 0.7902439024390245,\n",
       " 0.7867298578199053,\n",
       " 0.7980769230769232,\n",
       " 0.6602870813397129,\n",
       " 0.8095238095238095,\n",
       " 0.7307692307692307,\n",
       " 0.714975845410628,\n",
       " 0.7655502392344498,\n",
       " 0.7559808612440192,\n",
       " 0.6919431279620852,\n",
       " 0.7281553398058253,\n",
       " 0.7264150943396226,\n",
       " 0.8212560386473429,\n",
       " 0.7980769230769232,\n",
       " 0.7281553398058253,\n",
       " 0.7238095238095237,\n",
       " 0.8436018957345971,\n",
       " 0.6923076923076923,\n",
       " 0.7772511848341233,\n",
       " 0.8229665071770336,\n",
       " 0.7904761904761907,\n",
       " 0.8365384615384617,\n",
       " 0.6507177033492824,\n",
       " 0.7053140096618357,\n",
       " 0.7317073170731707,\n",
       " 0.8038277511961722,\n",
       " 0.6285714285714286,\n",
       " 0.7677725118483413,\n",
       " 0.6730769230769231,\n",
       " 0.6538461538461539,\n",
       " 0.7298578199052131,\n",
       " 0.7333333333333333,\n",
       " 0.7417840375586854,\n",
       " 0.7368421052631579,\n",
       " 0.7962085308056872,\n",
       " 0.7547169811320755,\n",
       " 0.7238095238095237,\n",
       " 0.7211538461538461,\n",
       " 0.7219512195121951,\n",
       " 0.7788461538461539,\n",
       " 0.7053140096618357,\n",
       " 0.6220095693779905,\n",
       " 0.784688995215311,\n",
       " 0.7169811320754716,\n",
       " 0.8252427184466018,\n",
       " 0.7109004739336493,\n",
       " 0.8653846153846154,\n",
       " 0.7393364928909952,\n",
       " 0.7014218009478673,\n",
       " 0.7358490566037735,\n",
       " 0.7632850241545893,\n",
       " 0.7559808612440192,\n",
       " 0.7307692307692307,\n",
       " 0.6540284360189573,\n",
       " 0.7177033492822966,\n",
       " 0.6985645933014353,\n",
       " 0.7087378640776698,\n",
       " 0.7403846153846154,\n",
       " 0.7211538461538461,\n",
       " 0.6666666666666666,\n",
       " 0.7333333333333333,\n",
       " 0.7729468599033816,\n",
       " 0.7417840375586854,\n",
       " 0.7707317073170732,\n",
       " 0.6376811594202898,\n",
       " 0.6857142857142856,\n",
       " 0.7902439024390245,\n",
       " 0.7014218009478673,\n",
       " 0.7632850241545893,\n",
       " 0.8173076923076923,\n",
       " 0.7677725118483413,\n",
       " 0.6376811594202898,\n",
       " 0.7547169811320755,\n",
       " 0.7238095238095237,\n",
       " 0.7358490566037735,\n",
       " 0.7788461538461539,\n",
       " 0.6794258373205742,\n",
       " 0.7830188679245284,\n",
       " 0.7393364928909952,\n",
       " 0.7904761904761907,\n",
       " 0.6407766990291263,\n",
       " 0.7809523809523811,\n",
       " 0.7317073170731707,\n",
       " 0.7766990291262137,\n",
       " 0.6796116504854369,\n",
       " 0.6761904761904762,\n",
       " 0.7368421052631579,\n",
       " 0.6602870813397129,\n",
       " 0.8349514563106797,\n",
       " 0.7655502392344498,\n",
       " 0.75,\n",
       " 0.7980769230769232,\n",
       " 0.7488151658767772,\n",
       " 0.6509433962264151,\n",
       " 0.7751196172248804,\n",
       " 0.7281553398058253,\n",
       " 0.6731707317073171,\n",
       " 0.7632850241545893,\n",
       " 0.7619047619047619,\n",
       " 0.714975845410628,\n",
       " 0.7142857142857143,\n",
       " 0.7609756097560976,\n",
       " 0.7830188679245284,\n",
       " 0.7047619047619048,\n",
       " 0.7142857142857143,\n",
       " 0.6796116504854369,\n",
       " 0.7142857142857143,\n",
       " 0.7488151658767772,\n",
       " 0.7830188679245284,\n",
       " 0.6729857819905213,\n",
       " 0.7942583732057417,\n",
       " 0.6698564593301435,\n",
       " 0.838095238095238,\n",
       " 0.6923076923076923,\n",
       " 0.6442307692307693,\n",
       " 0.7475728155339807,\n",
       " 0.7699530516431925,\n",
       " 0.7281553398058253,\n",
       " 0.6919431279620852,\n",
       " 0.7393364928909952,\n",
       " 0.7109004739336493,\n",
       " 0.7087378640776698,\n",
       " 0.6602870813397129,\n",
       " 0.6796116504854369,\n",
       " 0.8190476190476191,\n",
       " 0.6538461538461539,\n",
       " 0.7246376811594204,\n",
       " 0.7632850241545893,\n",
       " 0.6824644549763034,\n",
       " 0.7980769230769232,\n",
       " 0.7081339712918661,\n",
       " 0.6666666666666666,\n",
       " 0.7203791469194313,\n",
       " 0.737864077669903,\n",
       " 0.6952380952380952,\n",
       " 0.7547169811320755,\n",
       " 0.6634615384615384,\n",
       " 0.6985645933014353,\n",
       " 0.7942583732057417,\n",
       " 0.6161137440758294,\n",
       " 0.7729468599033816,\n",
       " 0.6824644549763034,\n",
       " 0.7788461538461539,\n",
       " 0.7980769230769232,\n",
       " 0.7772511848341233,\n",
       " 0.7766990291262137,\n",
       " 0.6824644549763034,\n",
       " 0.7019230769230769,\n",
       " 0.6985645933014353,\n",
       " 0.7393364928909952,\n",
       " 0.6602870813397129,\n",
       " 0.7464114832535885,\n",
       " 0.7177033492822966,\n",
       " 0.7019230769230769,\n",
       " 0.7735849056603775,\n",
       " 0.7488151658767772,\n",
       " 0.8019323671497585,\n",
       " 0.820754716981132,\n",
       " 0.7536231884057971,\n",
       " 0.7393364928909952,\n",
       " 0.6380952380952382,\n",
       " 0.7203791469194313,\n",
       " 0.6760563380281691,\n",
       " 0.8461538461538461,\n",
       " 0.7830188679245284,\n",
       " 0.6893203883495145,\n",
       " 0.8365384615384617,\n",
       " 0.6666666666666666,\n",
       " 0.7246376811594204,\n",
       " 0.6634615384615384,\n",
       " 0.7203791469194313,\n",
       " 0.7475728155339807,\n",
       " 0.7403846153846154,\n",
       " 0.7572815533980582,\n",
       " 0.7809523809523811,\n",
       " 0.7488151658767772,\n",
       " 0.7464114832535885,\n",
       " 0.7632850241545893,\n",
       " 0.7081339712918661,\n",
       " 0.7619047619047619,\n",
       " 0.7488151658767772,\n",
       " 0.6794258373205742,\n",
       " 0.7281553398058253,\n",
       " 0.7655502392344498,\n",
       " 0.7655502392344498,\n",
       " 0.7115384615384616,\n",
       " 0.7184466019417476,\n",
       " 0.7464114832535885,\n",
       " 0.7735849056603775,\n",
       " 0.7464114832535885,\n",
       " 0.7523809523809524,\n",
       " 0.7641509433962264,\n",
       " 0.8173076923076923,\n",
       " 0.7109004739336493,\n",
       " 0.6601941747572816,\n",
       " 0.7788461538461539,\n",
       " 0.7177033492822966,\n",
       " 0.7053140096618357,\n",
       " 0.6796116504854369,\n",
       " 0.7317073170731707,\n",
       " 0.6956521739130435,\n",
       " 0.7714285714285715,\n",
       " 0.6601941747572816,\n",
       " 0.7081339712918661,\n",
       " 0.7572815533980582,\n",
       " 0.6919431279620852,\n",
       " 0.7488151658767772,\n",
       " 0.784688995215311,\n",
       " 0.7830188679245284,\n",
       " 0.7523809523809524,\n",
       " 0.7075471698113207,\n",
       " 0.7087378640776698,\n",
       " 0.7735849056603775,\n",
       " 0.7523809523809524,\n",
       " 0.7403846153846154,\n",
       " 0.6666666666666666,\n",
       " 0.7403846153846154,\n",
       " 0.7452830188679245,\n",
       " 0.7358490566037735,\n",
       " 0.7884615384615384,\n",
       " 0.7333333333333333,\n",
       " 0.7272727272727273,\n",
       " 0.75,\n",
       " 0.854368932038835,\n",
       " 0.714975845410628,\n",
       " 0.8133971291866028,\n",
       " 0.7177033492822966,\n",
       " 0.6666666666666666,\n",
       " 0.7669902912621359,\n",
       " 0.7464114832535885,\n",
       " 0.6761904761904762,\n",
       " 0.7047619047619048,\n",
       " 0.7428571428571428,\n",
       " 0.7559808612440192,\n",
       " 0.7177033492822966,\n",
       " 0.7087378640776698,\n",
       " 0.7793427230046949,\n",
       " 0.7707317073170732,\n",
       " 0.7014218009478673,\n",
       " 0.75,\n",
       " 0.6310679611650485,\n",
       " 0.7596153846153846,\n",
       " 0.7596153846153846,\n",
       " 0.6445497630331753,\n",
       " 0.7203791469194313,\n",
       " 0.8502415458937197,\n",
       " 0.714975845410628,\n",
       " 0.7924528301886793,\n",
       " 0.6919431279620852,\n",
       " 0.7867298578199053,\n",
       " 0.7559808612440192,\n",
       " 0.7867298578199053,\n",
       " 0.7692307692307694,\n",
       " 0.7177033492822966,\n",
       " 0.6634615384615384,\n",
       " 0.8133971291866028,\n",
       " 0.8095238095238095,\n",
       " 0.6981132075471698,\n",
       " 0.7619047619047619,\n",
       " 0.7751196172248804,\n",
       " 0.7677725118483413,\n",
       " 0.7632850241545893,\n",
       " 0.7942583732057417,\n",
       " 0.6698564593301435,\n",
       " 0.7177033492822966,\n",
       " 0.7488151658767772,\n",
       " 0.7619047619047619,\n",
       " 0.7559808612440192,\n",
       " 0.7904761904761907,\n",
       " 0.6346153846153846,\n",
       " 0.7142857142857143,\n",
       " 0.8325358851674641,\n",
       " 0.6729857819905213,\n",
       " 0.8076923076923077,\n",
       " 0.6857142857142856,\n",
       " 0.8133971291866028,\n",
       " 0.6952380952380952,\n",
       " 0.7523809523809524,\n",
       " 0.6796116504854369,\n",
       " 0.8229665071770336,\n",
       " 0.7109004739336493,\n",
       " 0.8,\n",
       " 0.7523809523809524,\n",
       " 0.7219512195121951,\n",
       " 0.7772511848341233,\n",
       " 0.6923076923076923,\n",
       " 0.6859903381642511,\n",
       " 0.6476190476190476,\n",
       " 0.7655502392344498,\n",
       " 0.7867298578199053,\n",
       " 0.7632850241545893,\n",
       " 0.7184466019417476,\n",
       " 0.6666666666666666,\n",
       " 0.7428571428571428,\n",
       " 0.7523809523809524,\n",
       " 0.6893203883495145,\n",
       " 0.7980769230769232,\n",
       " 0.7019230769230769,\n",
       " 0.8173076923076923,\n",
       " 0.7169811320754716,\n",
       " 0.7439613526570049,\n",
       " 0.6666666666666666,\n",
       " 0.8173076923076923,\n",
       " 0.7714285714285715,\n",
       " 0.6478873239436619,\n",
       " 0.7488151658767772,\n",
       " 0.6956521739130435,\n",
       " 0.6540284360189573,\n",
       " 0.7867298578199053,\n",
       " 0.6634615384615384,\n",
       " 0.7014218009478673,\n",
       " 0.6760563380281691,\n",
       " 0.6889952153110048,\n",
       " 0.7788461538461539,\n",
       " 0.7464114832535885,\n",
       " 0.7751196172248804,\n",
       " 0.7714285714285715,\n",
       " 0.7428571428571428,\n",
       " 0.7632850241545893,\n",
       " 0.7867298578199053,\n",
       " 0.7655502392344498,\n",
       " 0.6666666666666666,\n",
       " 0.7428571428571428,\n",
       " 0.7735849056603775,\n",
       " 0.8076923076923077,\n",
       " 0.6601941747572816,\n",
       " 0.7655502392344498,\n",
       " 0.7142857142857143,\n",
       " 0.6666666666666666,\n",
       " 0.6538461538461539,\n",
       " 0.6411483253588517,\n",
       " 0.6572769953051644,\n",
       " 0.6956521739130435,\n",
       " 0.7047619047619048,\n",
       " 0.7333333333333333,\n",
       " 0.7014218009478673,\n",
       " 0.8421052631578947,\n",
       " 0.7136150234741785,\n",
       " 0.676328502415459,\n",
       " 0.8190476190476191,\n",
       " 0.7547169811320755,\n",
       " 0.6889952153110048,\n",
       " 0.8,\n",
       " 0.7053140096618357,\n",
       " 0.8133971291866028,\n",
       " 0.7559808612440192,\n",
       " 0.7475728155339807,\n",
       " 0.7464114832535885,\n",
       " 0.7368421052631579,\n",
       " 0.6540284360189573,\n",
       " 0.6952380952380952,\n",
       " 0.784688995215311,\n",
       " 0.7729468599033816,\n",
       " 0.6952380952380952,\n",
       " 0.6826923076923077,\n",
       " 0.676328502415459,\n",
       " 0.7714285714285715,\n",
       " 0.7867298578199053,\n",
       " 0.7169811320754716,\n",
       " 0.7864077669902914,\n",
       " 0.6923076923076923,\n",
       " 0.7751196172248804,\n",
       " 0.7619047619047619,\n",
       " 0.8019323671497585,\n",
       " 0.7142857142857143,\n",
       " 0.7238095238095237,\n",
       " 0.8076923076923077,\n",
       " 0.7368421052631579,\n",
       " 0.6730769230769231,\n",
       " 0.7342995169082125,\n",
       " 0.7298578199052131,\n",
       " 0.7169811320754716,\n",
       " 0.8019323671497585,\n",
       " 0.6886792452830188,\n",
       " 0.6730769230769231,\n",
       " 0.7272727272727273,\n",
       " 0.7115384615384616,\n",
       " 0.6824644549763034,\n",
       " 0.7464114832535885,\n",
       " 0.8019323671497585,\n",
       " 0.6634615384615384,\n",
       " 0.75,\n",
       " 0.6698564593301435,\n",
       " 0.6985645933014353,\n",
       " 0.6985645933014353,\n",
       " 0.7109004739336493,\n",
       " 0.7417840375586854,\n",
       " 0.7393364928909952,\n",
       " 0.7177033492822966,\n",
       " 0.6792452830188679,\n",
       " 0.7641509433962264,\n",
       " 0.8056872037914692,\n",
       " 0.7677725118483413,\n",
       " 0.7342995169082125,\n",
       " 0.7439613526570049,\n",
       " 0.7177033492822966,\n",
       " 0.6796116504854369,\n",
       " 0.6919431279620852,\n",
       " 0.7403846153846154,\n",
       " 0.631578947368421,\n",
       " 0.6666666666666666,\n",
       " 0.7238095238095237,\n",
       " 0.7641509433962264,\n",
       " 0.6346153846153846,\n",
       " 0.7403846153846154,\n",
       " 0.7368421052631579,\n",
       " 0.7417840375586854,\n",
       " 0.6384976525821597,\n",
       " 0.8195121951219512,\n",
       " 0.7414634146341462,\n",
       " 0.6634615384615384,\n",
       " 0.6857142857142856,\n",
       " 0.6509433962264151,\n",
       " 0.7358490566037735,\n",
       " 0.7053140096618357,\n",
       " 0.7572815533980582,\n",
       " 0.6824644549763034,\n",
       " 0.7669902912621359,\n",
       " 0.7014218009478673,\n",
       " 0.782608695652174,\n",
       " 0.7699530516431925,\n",
       " 0.7014218009478673,\n",
       " 0.7511737089201879,\n",
       " 0.6570048309178744,\n",
       " 0.7014218009478673,\n",
       " 0.7729468599033816,\n",
       " 0.6796116504854369,\n",
       " 0.6666666666666666,\n",
       " 0.7864077669902914,\n",
       " 0.7177033492822966,\n",
       " 0.7809523809523811,\n",
       " 0.7692307692307694,\n",
       " 0.6926829268292682,\n",
       " 0.7368421052631579,\n",
       " 0.7677725118483413,\n",
       " 0.7047619047619048,\n",
       " 0.784688995215311,\n",
       " 0.6698564593301435,\n",
       " 0.6985645933014353,\n",
       " 0.7536231884057971,\n",
       " 0.6602870813397129,\n",
       " 0.782608695652174,\n",
       " 0.7053140096618357,\n",
       " 0.7830188679245284,\n",
       " 0.7368421052631579,\n",
       " 0.7488151658767772,\n",
       " 0.6796116504854369,\n",
       " 0.8405797101449275,\n",
       " 0.6792452830188679,\n",
       " 0.6470588235294118,\n",
       " 0.75,\n",
       " 0.6985645933014353,\n",
       " 0.7393364928909952,\n",
       " 0.7596153846153846,\n",
       " 0.7121951219512195,\n",
       " 0.7238095238095237,\n",
       " 0.6538461538461539,\n",
       " 0.6571428571428571,\n",
       " 0.7298578199052131,\n",
       " 0.7342995169082125,\n",
       " 0.6473429951690821,\n",
       " 0.8019323671497585,\n",
       " 0.8390243902439024,\n",
       " 0.8076923076923077,\n",
       " 0.8173076923076923,\n",
       " 0.6985645933014353,\n",
       " 0.6796116504854369,\n",
       " 0.7962085308056872,\n",
       " 0.676328502415459,\n",
       " 0.7464114832535885,\n",
       " 0.8019323671497585,\n",
       " 0.7211538461538461,\n",
       " 0.7383177570093458,\n",
       " 0.7368421052631579,\n",
       " 0.7475728155339807,\n",
       " 0.6603773584905661,\n",
       " 0.7075471698113207,\n",
       " 0.7142857142857143,\n",
       " 0.7452830188679245,\n",
       " 0.7632850241545893,\n",
       " 0.7358490566037735,\n",
       " 0.6571428571428571,\n",
       " 0.8341232227488152,\n",
       " 0.7272727272727273,\n",
       " 0.676328502415459,\n",
       " 0.7019230769230769,\n",
       " 0.7307692307692307,\n",
       " 0.8269230769230769,\n",
       " 0.7559808612440192,\n",
       " 0.7053140096618357,\n",
       " 0.6699029126213593,\n",
       " 0.7464114832535885,\n",
       " 0.7511737089201879,\n",
       " 0.737864077669903,\n",
       " 0.8173076923076923,\n",
       " 0.6635071090047393,\n",
       " 0.7333333333333333,\n",
       " 0.7464114832535885,\n",
       " 0.7884615384615384,\n",
       " 0.7439613526570049,\n",
       " 0.7904761904761907,\n",
       " 0.7632850241545893,\n",
       " 0.6761904761904762,\n",
       " 0.6854460093896714,\n",
       " 0.7735849056603775,\n",
       " 0.8461538461538461,\n",
       " 0.6886792452830188,\n",
       " 0.7729468599033816,\n",
       " 0.7751196172248804,\n",
       " 0.6985645933014353,\n",
       " 0.6666666666666666,\n",
       " 0.7203791469194313,\n",
       " 0.7368421052631579,\n",
       " 0.7751196172248804,\n",
       " 0.7081339712918661,\n",
       " 0.7714285714285715,\n",
       " 0.6504854368932039,\n",
       " 0.7729468599033816,\n",
       " 0.784688995215311,\n",
       " 0.7523809523809524,\n",
       " 0.7475728155339807,\n",
       " 0.7464114832535885,\n",
       " 0.7464114832535885,\n",
       " 0.8,\n",
       " 0.6407766990291263,\n",
       " 0.8252427184466018,\n",
       " 0.7559808612440192,\n",
       " 0.6981132075471698,\n",
       " 0.6889952153110048,\n",
       " 0.6857142857142856,\n",
       " 0.7081339712918661,\n",
       " 0.7428571428571428,\n",
       " 0.7053140096618357,\n",
       " 0.8212560386473429,\n",
       " 0.7536231884057971,\n",
       " 0.6794258373205742,\n",
       " 0.7019230769230769,\n",
       " 0.8038277511961722,\n",
       " 0.714975845410628,\n",
       " 0.7254901960784315,\n",
       " 0.7439613526570049,\n",
       " 0.8133971291866028,\n",
       " 0.7109004739336493,\n",
       " 0.7692307692307694,\n",
       " 0.6542056074766356,\n",
       " 0.6919431279620852,\n",
       " 0.7751196172248804,\n",
       " 0.7582938388625592,\n",
       " 0.8229665071770336,\n",
       " 0.8229665071770336,\n",
       " 0.6952380952380952,\n",
       " 0.7464114832535885,\n",
       " 0.6380952380952382,\n",
       " 0.7019230769230769,\n",
       " 0.6919431279620852,\n",
       " 0.6919431279620852,\n",
       " 0.6255924170616113,\n",
       " 0.7014218009478673,\n",
       " 0.6919431279620852,\n",
       " 0.7788461538461539,\n",
       " 0.6952380952380952,\n",
       " 0.7632850241545893,\n",
       " 0.7136150234741785,\n",
       " 0.7804878048780488,\n",
       " 0.7109004739336493,\n",
       " 0.839622641509434,\n",
       " 0.7439613526570049,\n",
       " 0.7655502392344498,\n",
       " 0.6220095693779905,\n",
       " 0.7523809523809524,\n",
       " 0.7109004739336493,\n",
       " 0.7272727272727273,\n",
       " 0.7203791469194313,\n",
       " 0.7177033492822966,\n",
       " 0.7582938388625592,\n",
       " 0.7464114832535885,\n",
       " 0.6857142857142856,\n",
       " 0.7699530516431925,\n",
       " 0.7211538461538461,\n",
       " 0.6538461538461539,\n",
       " 0.7572815533980582,\n",
       " 0.6183574879227053,\n",
       " 0.7109004739336493,\n",
       " 0.8476190476190476,\n",
       " 0.7735849056603775,\n",
       " 0.7272727272727273,\n",
       " 0.7264150943396226,\n",
       " 0.7053140096618357,\n",
       " 0.7109004739336493,\n",
       " 0.7272727272727273,\n",
       " 0.6666666666666666,\n",
       " 0.6504854368932039,\n",
       " 0.6666666666666666,\n",
       " 0.6889952153110048,\n",
       " 0.8557692307692308,\n",
       " 0.6857142857142856,\n",
       " 0.6919431279620852,\n",
       " 0.7962085308056872,\n",
       " 0.7559808612440192,\n",
       " 0.8076923076923077,\n",
       " 0.6952380952380952,\n",
       " 0.8325358851674641,\n",
       " 0.6981132075471698,\n",
       " 0.7547169811320755,\n",
       " 0.7075471698113207,\n",
       " 0.7692307692307694,\n",
       " 0.6473429951690821,\n",
       " 0.7619047619047619,\n",
       " 0.676328502415459,\n",
       " 0.6952380952380952,\n",
       " 0.6956521739130435,\n",
       " 0.6376811594202898,\n",
       " 0.6540284360189573,\n",
       " 0.7393364928909952,\n",
       " 0.6952380952380952,\n",
       " 0.6603773584905661,\n",
       " 0.7536231884057971,\n",
       " 0.7692307692307694,\n",
       " 0.7559808612440192,\n",
       " 0.6889952153110048,\n",
       " 0.7042253521126761,\n",
       " 0.838095238095238,\n",
       " 0.6507177033492824,\n",
       " 0.6824644549763034,\n",
       " 0.7922705314009661,\n",
       " 0.6285714285714286,\n",
       " 0.7655502392344498,\n",
       " 0.6504854368932039,\n",
       " 0.7572815533980582,\n",
       " 0.7864077669902914,\n",
       " 0.7641509433962264,\n",
       " 0.6919431279620852,\n",
       " 0.7523809523809524,\n",
       " 0.6634615384615384,\n",
       " 0.7368421052631579,\n",
       " 0.7980769230769232,\n",
       " 0.6794258373205742,\n",
       " 0.6889952153110048,\n",
       " 0.6666666666666666,\n",
       " 0.7019230769230769,\n",
       " 0.6602870813397129,\n",
       " 0.8,\n",
       " 0.7281553398058253,\n",
       " 0.7669902912621359,\n",
       " 0.7692307692307694,\n",
       " 0.7488151658767772,\n",
       " 0.7358490566037735,\n",
       " 0.6859903381642511,\n",
       " 0.6923076923076923,\n",
       " 0.7264150943396226,\n",
       " 0.7156862745098039,\n",
       " 0.7596153846153846,\n",
       " 0.7619047619047619,\n",
       " 0.7358490566037735,\n",
       " 0.6255924170616113,\n",
       " 0.7751196172248804,\n",
       " 0.6985645933014353,\n",
       " 0.8803827751196172,\n",
       " 0.7619047619047619,\n",
       " 0.6509433962264151,\n",
       " 0.6923076923076923,\n",
       " 0.6889952153110048,\n",
       " 0.7942583732057417,\n",
       " 0.6602870813397129,\n",
       " 0.6730769230769231,\n",
       " 0.6761904761904762,\n",
       " 0.7669902912621359,\n",
       " 0.7714285714285715,\n",
       " 0.7788461538461539,\n",
       " 0.7714285714285715,\n",
       " 0.8056872037914692,\n",
       " 0.8056872037914692,\n",
       " 0.6602870813397129,\n",
       " 0.7087378640776698,\n",
       " 0.8018867924528302,\n",
       " 0.7115384615384616,\n",
       " 0.75,\n",
       " 0.7439613526570049,\n",
       " 0.7751196172248804,\n",
       " 0.7922705314009661,\n",
       " 0.7904761904761907,\n",
       " 0.6952380952380952,\n",
       " 0.7922705314009661,\n",
       " 0.7559808612440192,\n",
       " 0.6571428571428571,\n",
       " 0.8,\n",
       " 0.7980769230769232,\n",
       " 0.6857142857142856,\n",
       " 0.7342995169082125,\n",
       " 0.737864077669903,\n",
       " 0.7439613526570049,\n",
       " 0.7751196172248804,\n",
       " 0.7087378640776698,\n",
       " 0.8502415458937197,\n",
       " 0.7793427230046949,\n",
       " 0.7523809523809524,\n",
       " 0.7307692307692307,\n",
       " 0.7053140096618357,\n",
       " 0.7488151658767772,\n",
       " 0.7942583732057417,\n",
       " 0.7053140096618357,\n",
       " 0.6478873239436619,\n",
       " 0.7203791469194313,\n",
       " 0.6571428571428571,\n",
       " 0.8038277511961722,\n",
       " 0.7609756097560976,\n",
       " 0.7692307692307694,\n",
       " 0.6889952153110048,\n",
       " 0.8269230769230769,\n",
       " 0.6504854368932039,\n",
       " 0.7582938388625592,\n",
       " 0.7523809523809524,\n",
       " 0.7014218009478673,\n",
       " 0.7677725118483413,\n",
       " 0.7962085308056872,\n",
       " 0.7596153846153846,\n",
       " 0.7677725118483413,\n",
       " 0.6666666666666666,\n",
       " 0.7142857142857143,\n",
       " 0.7572815533980582,\n",
       " 0.7809523809523811,\n",
       " 0.7169811320754716,\n",
       " 0.7729468599033816,\n",
       " 0.7042253521126761,\n",
       " 0.7452830188679245,\n",
       " 0.7081339712918661,\n",
       " 0.7523809523809524,\n",
       " 0.7523809523809524,\n",
       " 0.7772511848341233,\n",
       " 0.7024390243902439,\n",
       " 0.7428571428571428,\n",
       " 0.6923076923076923,\n",
       " 0.6635071090047393,\n",
       " 0.7053140096618357,\n",
       " 0.7246376811594204,\n",
       " 0.7368421052631579,\n",
       " 0.676328502415459,\n",
       " 0.7669902912621359,\n",
       " 0.75,\n",
       " 0.7081339712918661,\n",
       " 0.7788461538461539,\n",
       " 0.7439613526570049,\n",
       " 0.8,\n",
       " 0.7439613526570049,\n",
       " 0.6504854368932039,\n",
       " 0.6666666666666666,\n",
       " 0.7609756097560976,\n",
       " 0.6699029126213593,\n",
       " 0.8056872037914692,\n",
       " 0.6948356807511737,\n",
       " 0.7342995169082125,\n",
       " 0.7014218009478673,\n",
       " 0.7136150234741785,\n",
       " 0.6796116504854369,\n",
       " 0.6857142857142856,\n",
       " 0.7450980392156863,\n",
       " 0.7512195121951221,\n",
       " 0.7203791469194313,\n",
       " 0.6540284360189573,\n",
       " 0.7393364928909952,\n",
       " 0.7075471698113207,\n",
       " 0.7596153846153846,\n",
       " 0.7087378640776698,\n",
       " 0.6985645933014353,\n",
       " 0.7053140096618357,\n",
       " 0.7669902912621359,\n",
       " 0.7177033492822966,\n",
       " 0.6635071090047393,\n",
       " 0.6570048309178744,\n",
       " 0.7368421052631579,\n",
       " 0.6601941747572816,\n",
       " 0.7075471698113207,\n",
       " 0.676328502415459,\n",
       " 0.7333333333333333,\n",
       " 0.7609756097560976,\n",
       " 0.6376811594202898,\n",
       " 0.7632850241545893,\n",
       " 0.7476635514018692,\n",
       " 0.7238095238095237,\n",
       " 0.7177033492822966,\n",
       " 0.6956521739130435,\n",
       " 0.7368421052631579,\n",
       " 0.7075471698113207,\n",
       " 0.6952380952380952,\n",
       " 0.6407766990291263,\n",
       " 0.7342995169082125,\n",
       " 0.7298578199052131,\n",
       " 0.7246376811594204,\n",
       " 0.6540284360189573,\n",
       " 0.7298578199052131,\n",
       " 0.7788461538461539,\n",
       " 0.7393364928909952,\n",
       " 0.6824644549763034,\n",
       " 0.7751196172248804,\n",
       " 0.7087378640776698,\n",
       " 0.7464114832535885,\n",
       " 0.6439024390243901,\n",
       " 0.6889952153110048,\n",
       " 0.7238095238095237,\n",
       " 0.8133971291866028,\n",
       " 0.8173076923076923,\n",
       " 0.7203791469194313,\n",
       " 0.6666666666666666,\n",
       " 0.737864077669903,\n",
       " 0.7042253521126761,\n",
       " 0.7246376811594204,\n",
       " 0.6889952153110048,\n",
       " 0.7087378640776698,\n",
       " 0.7428571428571428,\n",
       " 0.7655502392344498,\n",
       " 0.7352941176470589,\n",
       " 0.8056872037914692,\n",
       " 0.7081339712918661,\n",
       " 0.7307692307692307,\n",
       " 0.6796116504854369,\n",
       " 0.7804878048780488,\n",
       " 0.7177033492822966,\n",
       " 0.7238095238095237,\n",
       " 0.7922705314009661,\n",
       " 0.8325358851674641,\n",
       " 0.7714285714285715,\n",
       " 0.6476190476190476,\n",
       " 0.7238095238095237,\n",
       " 0.8,\n",
       " 0.7358490566037735,\n",
       " 0.7980769230769232,\n",
       " 0.8,\n",
       " 0.7980769230769232,\n",
       " 0.8309178743961352,\n",
       " 0.7136150234741785,\n",
       " 0.8056872037914692,\n",
       " 0.7830188679245284,\n",
       " 0.7735849056603775,\n",
       " 0.7053140096618357,\n",
       " 0.7087378640776698,\n",
       " 0.7751196172248804,\n",
       " 0.7729468599033816,\n",
       " 0.6730769230769231,\n",
       " 0.784688995215311,\n",
       " 0.7830188679245284,\n",
       " 0.7081339712918661,\n",
       " 0.7632850241545893,\n",
       " 0.7230046948356808,\n",
       " 0.8,\n",
       " 0.6698564593301435,\n",
       " 0.838095238095238,\n",
       " 0.6889952153110048,\n",
       " 0.6411483253588517,\n",
       " 0.8113207547169812,\n",
       " 0.7902439024390245,\n",
       " 0.6509433962264151,\n",
       " 0.6666666666666666,\n",
       " 0.7452830188679245,\n",
       " 0.782608695652174,\n",
       " 0.6602870813397129,\n",
       " 0.7342995169082125,\n",
       " 0.7238095238095237,\n",
       " 0.6376811594202898,\n",
       " 0.7169811320754716,\n",
       " 0.782608695652174,\n",
       " 0.7169811320754716,\n",
       " 0.6985645933014353,\n",
       " 0.6886792452830188,\n",
       " 0.839622641509434,\n",
       " 0.6603773584905661,\n",
       " 0.6829268292682926,\n",
       " 0.7439613526570049,\n",
       " 0.7452830188679245,\n",
       " 0.6729857819905213,\n",
       " 0.7692307692307694,\n",
       " 0.7196261682242991,\n",
       " 0.6602870813397129,\n",
       " 0.7582938388625592,\n",
       " 0.6794258373205742,\n",
       " 0.8,\n",
       " 0.7677725118483413,\n",
       " 0.6919431279620852,\n",
       " 0.784688995215311,\n",
       " 0.7238095238095237,\n",
       " 0.6893203883495145,\n",
       " 0.7439613526570049,\n",
       " 0.7272727272727273,\n",
       " 0.7488151658767772,\n",
       " 0.7281553398058253,\n",
       " 0.6990291262135924,\n",
       " 0.7596153846153846,\n",
       " 0.6372549019607843,\n",
       " 0.7512195121951221,\n",
       " 0.6699029126213593,\n",
       " 0.6698564593301435,\n",
       " 0.7475728155339807,\n",
       " 0.7677725118483413,\n",
       " 0.7203791469194313,\n",
       " 0.7169811320754716,\n",
       " 0.7246376811594204,\n",
       " 0.7358490566037735,\n",
       " 0.7788461538461539,\n",
       " 0.7342995169082125,\n",
       " 0.6698564593301435,\n",
       " 0.6956521739130435,\n",
       " 0.6990291262135924,\n",
       " 0.7511737089201879,\n",
       " 0.7729468599033816,\n",
       " 0.6504854368932039,\n",
       " 0.8056872037914692,\n",
       " 0.7961165048543689,\n",
       " 0.7428571428571428,\n",
       " 0.7547169811320755,\n",
       " 0.8212560386473429,\n",
       " 0.6796116504854369,\n",
       " 0.631578947368421,\n",
       " 0.6761904761904762,\n",
       " 0.7464114832535885,\n",
       " 0.7238095238095237,\n",
       " 0.7142857142857143,\n",
       " 0.6826923076923077,\n",
       " 0.7307692307692307,\n",
       " 0.7536231884057971,\n",
       " 0.6796116504854369,\n",
       " 0.6952380952380952,\n",
       " 0.6952380952380952,\n",
       " 0.7729468599033816,\n",
       " 0.7075471698113207,\n",
       " 0.7428571428571428,\n",
       " 0.782608695652174,\n",
       " 0.7047619047619048,\n",
       " 0.7368421052631579,\n",
       " 0.7655502392344498,\n",
       " 0.7677725118483413,\n",
       " 0.6985645933014353,\n",
       " 0.6923076923076923,\n",
       " 0.6923076923076923,\n",
       " 0.6634615384615384,\n",
       " 0.6761904761904762,\n",
       " 0.6255924170616113,\n",
       " 0.6923076923076923,\n",
       " 0.7298578199052131,\n",
       " 0.6760563380281691,\n",
       " 0.7439613526570049,\n",
       " 0.7254901960784315,\n",
       " 0.7464114832535885,\n",
       " 0.6985645933014353,\n",
       " 0.7203791469194313,\n",
       " 0.7772511848341233,\n",
       " 0.7307692307692307,\n",
       " 0.7523809523809524,\n",
       " 0.7211538461538461,\n",
       " 0.7596153846153846,\n",
       " 0.7439613526570049,\n",
       " 0.6473429951690821,\n",
       " 0.7281553398058253,\n",
       " 0.7238095238095237,\n",
       " 0.7523809523809524,\n",
       " 0.7536231884057971,\n",
       " 0.8019323671497585,\n",
       " 0.6923076923076923,\n",
       " 0.6822429906542056,\n",
       " 0.6285714285714286,\n",
       " 0.7596153846153846,\n",
       " 0.676328502415459,\n",
       " 0.6886792452830188,\n",
       " 0.7053140096618357,\n",
       " 0.7536231884057971,\n",
       " 0.7559808612440192,\n",
       " 0.7203791469194313,\n",
       " 0.8056872037914692,\n",
       " 0.7655502392344498,\n",
       " ...]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.19632425],\n",
       "       [0.19632425, 1.        ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(probs,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9944280da0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99442be7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(f1,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9927866f98>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99442a15c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(f1,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
